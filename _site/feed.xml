<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jisoo Lee</title>
    <atom:link href="/blog-site/feed.xml" rel="self" type="application/rss+xml"/>
    <link>http://localhost:4000/blog-site/</link>
    <description>Data analytics blog</description>
    <pubDate>Fri, 23 Sep 2022 09:13:08 -0700</pubDate>
    
      <item>
        <title>Implementing Hierarchical and KMeans Clustering on Principal Components</title>
        <link>/blog-site/2022/09/18/implementing-hierarchical-and-kmeans-clustering-on-principal-components.html</link>
        <guid isPermaLink="true">/blog-site/2022/09/18/implementing-hierarchical-and-kmeans-clustering-on-principal-components.html</guid>
        <description>&lt;p&gt;Let’s say, you want to identify groups of customers to treat each group with different strategies. If you have a dataset containing features describing customers’ various aspects such as age, occupation, living area, spending behavior and so on, clustering methods can help you figure it out. Hierarchical clustering and KMeans clustering combined with PCA (Principal Component Analysis) are often used in various clustering methods. In this post, I will present how to implement hierarchical clustering, KMeans clustering, and both on PCA features, using SciPy and Scikit-learn libraries in Python.&lt;/p&gt;

&lt;p&gt;This description is guided by Asish Biswas’ worked example [1, 2, 3], which aims to show you how to apply the STP (Segmentation, Targeting, Positioning) marketing framework. The dataset used contains demographic information of 2000 customers, as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sex – categorical, 0: male, 1: female&lt;/li&gt;
  &lt;li&gt;Marital status – categorical, 0: single, 1: non-single&lt;/li&gt;
  &lt;li&gt;Age – numerical&lt;/li&gt;
  &lt;li&gt;Education – categorical, 0: unknown, 1: high school, 2: university, 3: graduate&lt;/li&gt;
  &lt;li&gt;Income – numerical&lt;/li&gt;
  &lt;li&gt;Occupation – categorical, 0: unemployed, 1: official, 2: management&lt;/li&gt;
  &lt;li&gt;Settlement size – categorical, 0: small city, 1: mid city, 2: big city&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will first talk about initial data exploration and do preprocessing. Then, I will show implementation of the clusterings one by one and then compare results of them. You can find full implementation and code in my &lt;a href=&quot;https://github.com/jisooleeworks/blog-code/blob/main/implement_clustering.ipynb&quot;&gt;github repository&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;data-exploration-and-preprocessing&quot;&gt;Data Exploration and Preprocessing&lt;/h1&gt;
&lt;p&gt;Let’s first look at several rows of the dataset to get familar with it.&lt;/p&gt;

&lt;table style=&quot;border: 1; height: 180px;&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Sex&lt;/th&gt;
      &lt;th&gt;Marital status&lt;/th&gt;
      &lt;th&gt;Age&lt;/th&gt;
      &lt;th&gt;Education&lt;/th&gt;
      &lt;th&gt;Income&lt;/th&gt;
      &lt;th&gt;Occupation&lt;/th&gt;
      &lt;th&gt;Settlement size&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ID&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;100000001&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;67&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;124670&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;100000002&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;150773&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;100000003&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;49&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;89210&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;100000004&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;45&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;171565&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;100000005&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;53&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;149031&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I’ve drawn plots to check distribution of each variable and relationship between pairs of the variables.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/org_pairplot.png&quot; alt=&quot;Individual Distribution and Pairwise Relationship&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/org_correlation_heatmap.png&quot; alt=&quot;Correlation Matrix&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The plots reveal existence of several correlations including relationship between the age and the education and between the occupation and the income.&lt;/p&gt;

&lt;p&gt;Since distance-based clustering methods are affected by difference of variable scales, I applied  standardization to the dataset. I picked the standardization as I will do PCA.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StandardScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data_sd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;hierarchical-clustering&quot;&gt;Hierarchical Clustering&lt;/h1&gt;
&lt;p&gt;I apply euclidean distance for a distance measure and Ward´s linkage for a linkage function.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.cluster&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hierarchy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.spatial&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;linkages_sd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hierarchy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linkage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;ward&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hierarchy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dendrogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linkages_sd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;show_leaf_counts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;no_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hierarchical Clustering Dendrogram&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/hc_dendrogram.png&quot; alt=&quot;Hierarchical Clustering Dendrogram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The  dendrogram hints that making four groups may be reasonable.&lt;/p&gt;

&lt;h1 id=&quot;kmeans-clustering&quot;&gt;KMeans Clustering&lt;/h1&gt;
&lt;p&gt;To determine the optimal number of clusters, I draw an elbow plot that tells the degree of variance explained according to the different number of clusters.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;distortions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_clusters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_clusters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;centroids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inertia&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k_means&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_clusters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;distortions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inertia&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;labels_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;elbow_plot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;num_clusters&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_clusters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                            &lt;span class=&quot;s&quot;&gt;&apos;distortions&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distortions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Number of clusters&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Sum of squared distances&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elbow_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;o&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xticks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_clusters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;KMeans Clustering Elbow Graph&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/km_elbow.png&quot; alt=&quot;Elbow Graph of KMeans Clustering&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If comparing the magnitude of the slope between two numbers, it tends to remain the same from four. So, it is considered best to make four groups.&lt;/p&gt;

&lt;h1 id=&quot;pca&quot;&gt;PCA&lt;/h1&gt;
&lt;p&gt;In identifying principal components, let’s first import the sklearn library and create a PCA object with the scaled dataset.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decomposition&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decomposition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We draw a plot that shows the degree of the explained variance accumulated with additional component.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;o&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Number of components&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Cumulative explained aariance&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Cumulative Explained Variance by Number of Principal Components&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/pca_explanation.png&quot; alt=&quot;Correlation Matrix for Principal Components and Original Features&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above plot, we can see that the first three components explain 80% of the variance, which is considered acceptable [4]. So, I make an instance of the model with three for a number of components and fit it on the dataset. Then, I make a dataframe and heatmap with it to see a loading score of each variable for each component.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decomposition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PCA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df_pca_components&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;components_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;component 1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;component 2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;component 3&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heatmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_pca_components&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;RdBu_r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;annot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Correlation Matrix for Principal Components and Original Features&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/pca_correlation_heatmap.png&quot; alt=&quot;Correlation Matrix for Principal Components and Original Features&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the above heatmap, we can see that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Component 1 has a positive association with Income, Occupation, and Settlement Size&lt;/li&gt;
  &lt;li&gt;Component 2 has a positive association with Sex, Martital Status, and Education&lt;/li&gt;
  &lt;li&gt;Component 3 has a positive association with Age and a negative association with Martital Status and Occupation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hierarchical-clustering-and-kmeans-clustering-on-pca-features&quot;&gt;Hierarchical Clustering and KMeans Clustering on PCA Features&lt;/h1&gt;
&lt;p&gt;Let’t run a hierarchical clustering model and KMeans on the three principal components.&lt;/p&gt;

&lt;p&gt;I first obtain a new dataset by transforming the original scaled dataset on the three principal components and then run a model on it.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pca_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_sd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;linkages_pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hierarchy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linkage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pca_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;ward&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I do not bring a dendrogram of the result here; you can find it in my &lt;a href=&quot;https://github.com/jisooleeworks/blog-code/blob/main/implement_clustering.ipynb&quot;&gt;github repository&lt;/a&gt;. If you see it, you will find that making four groups is a reasonable decision.&lt;/p&gt;

&lt;p&gt;I plot the segments with respect to the first two components as below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;labels_hc_pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hierarchy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fcluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linkages_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;maxclust&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df_hc_pca&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pca_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df_hc_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;component 1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;component 2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;component 3&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df_hc_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;label&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels_hc_pca&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatterplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_hc_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;component 1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_hc_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;component 2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_hc_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;label&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;palette&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;b&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;m&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;g&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Hierarchical Clustering on Principle Components&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/hc_pca_scatter.png&quot; alt=&quot;Hierarchical Clustering on Principle Components&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I think the segments are well seperated in overall, although the label 4 overlaps the label 2 and 3 a bit.&lt;/p&gt;

&lt;h1 id=&quot;kmeans-clustering-on-pca-features&quot;&gt;KMeans Clustering on PCA Features&lt;/h1&gt;
&lt;p&gt;Finally, we run a KMeans clustering model on the three principal components. Based on the elbow graph that you can find in the github repository, I pick four as a number of clusters.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;centroids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inertia&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k_means&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pca_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_clusters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/km_pca_scatter.png&quot; alt=&quot;Hierarchical Clustering on Principle Components&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;comparison&quot;&gt;Comparison&lt;/h1&gt;
&lt;p&gt;Now, we have four different versions of segmentation on the dataset. Below is a summary of each version (mean value of each variable for each cluster), in order of hierarchical clustering, KMeans clusteirng, hierarchical clustering with PCA, and KMeans clustering with PCA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/four_results.png&quot; alt=&quot;Correlation Matrix for Principal Components and Original Features&quot; width=&quot;60%&quot; height=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Before getting the above, I’ve printed out a raw result of each version, done some eyeballing, and re-ordered it based on the age and income variable so that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Label 1, the youngest and the third level of income&lt;/li&gt;
  &lt;li&gt;Label 2, in the middle level in age and the lowest level of income&lt;/li&gt;
  &lt;li&gt;Label 3, in the mdidle level in the age and the second level of income&lt;/li&gt;
  &lt;li&gt;Label 4, the eldest and the the top level of income&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This segmentatation accord with all the four versions; e.g., the youngest group’s income rank is third in all the versions. The mean age values are simiar each other. But the mean income values are fairly much different. As well, we can see discordance between the versions in the other variables.&lt;/p&gt;

&lt;p&gt;I am going to do further analysis on the segmentation done with the Kmeans clustering on principal components.&lt;/p&gt;

&lt;h1 id=&quot;exploration-of-segments&quot;&gt;Exploration of Segments&lt;/h1&gt;
&lt;p&gt;In this section, I will look into the segments created with the Kmeans clustering on principal components to identify characteristics of each of them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/seg_age_income.png&quot; alt=&quot;Age and Income Distribution by Segment&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we compare the median values of the Age, Segment 4 is distinctively high, Segment 2 and 3 are similar, and Segment 1 is the lowest. However, we can see their overlapping with the long whiskers and outliers. The Income plot confirms that Segment 4 is higher than the others, especially Segment 3. However, we can see the wide range of Segment 4 with some extreme outliers on the top and the minimum value lower than Segment 3’s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/seg_uni_count.png&quot; alt=&quot;Count by Segment&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/seg_bi_count1.png&quot; alt=&quot;Count by Two Variables and Segment 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/seg_bi_count2.png&quot; alt=&quot;Count by Two Variables and Segment 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The most distinctive aspect of Segment 1 is that they are mostly high school graduated non-single women in their late 20s. Two-thirds of them live in the small city, half of which have no job. A third live in the mid-sized city with ‘official’ jobs.&lt;/p&gt;

&lt;p&gt;Most people of Segment 2 are single living in the small city in the mid 30s. Two-thirds are male and a third, female. Two-thirds are unemployed.&lt;/p&gt;

&lt;p&gt;People of Segment 3 are male living the mid or big-sized city with job. The majority is single.&lt;/p&gt;

&lt;p&gt;All people of Segment 4 were graduated from university or the higher. Two-thirds are non-single, and a third, single. The sex ratio is one to one. They distributed to all the sizes of the city although more people live in the mid or big-sized city.&lt;/p&gt;

&lt;h1 id=&quot;wrapping-up&quot;&gt;Wrapping Up&lt;/h1&gt;
&lt;p&gt;I have showed how to implement the main clustering methods, hierarchical and KMean clustering combined with PCA. When use the hierarchical clustering, you find out the appropriate number of clusters by looking at a dendrogram. If use the KMean clustering, the elbow graph can help you pick the number of clusters. We have found that the obtained segments are distintive each other, and it suggests usefulness of the clustering methods in segmenting customers.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Asish Biswas. &lt;a href=&quot;https://medium.com/towards-data-science/customer-segmentation-with-python-implementing-stp-framework-part-1-5c2d93066f82&quot;&gt;Customer Segmentation with Python (Implementing STP framework - part 1/5)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Asish Biswas. &lt;a href=&quot;https://medium.com/towards-data-science/customer-segmentation-with-python-implementing-stp-framework-part-2-689b81a7e86d&quot;&gt;Customer Segmentation with Python (Implementing STP Framework - Part 2/5&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Asish Biswas. &lt;a href=&quot;https://medium.com/towards-data-science/customer-segmentation-with-python-implementing-stp-framework-part-3-e81a79181d07&quot;&gt;Customer Segmentation with Python (Implementing STP Framework - Part 3/5)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Elitsa Kaloyanova. &lt;a href=&quot;https://365datascience.com/tutorials/python-tutorials/principal-components-analysis/&quot;&gt;What Is Principal Components Analysis?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 18 Sep 2022 00:00:00 -0700</pubDate>
      </item>
    
      <item>
        <title>Feature Scaling, Min-Max and Z-Score Normalization</title>
        <link>/blog-site/2022/09/09/feature-scaling-min-max-z-score.html</link>
        <guid isPermaLink="true">/blog-site/2022/09/09/feature-scaling-min-max-z-score.html</guid>
        <description>&lt;p&gt;Feature scaling is making features take similar ranges of values. In this post, I will talk about when it is necessary and then look into two most frequently mentioned methods, min-max normalization and z-score nomalization. Finally, several other tehniques will be introduced briefly including mean normalization, robust scalar, and scaling to unit length. Examples accompany to help understand effects of each method, which are made with the Python and scikit-learn APIs.&lt;/p&gt;

&lt;p&gt;Before diving into details, I would like to talk about the confusion due to naming discrepancies. Given the below, it seems that ‘scaling’ is ‘normalizing’, and ‘normalizing’ is ‘scaling’:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“Feature scaling is a method used to normalize the range of independent variables or features of data.” [1]&lt;/li&gt;
  &lt;li&gt;“The goal of normalization is to transform features to be on a similar scale.” [2]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;People often use them interchangeably (e.g, Min-Max Normalization [3] and Min-Max Scaling [4])&lt;/p&gt;

&lt;p&gt;Meanwhile, we can find many documents online that use the term, ‘normalization’, to indicate a particular scaling/normalizing method that is often called ‘min-max scaling’ or ‘min-max normalization’, typically contrasting it to other scaling/normalizing method, ‘standardization’, which is also called ‘z-score normalization’ [5][6][7].&lt;/p&gt;

&lt;p&gt;Given the inconsistent naming, we may need some extra info to figure out what exactly it means when we run into the term, ‘normalization’, in materials dealing with feature scaling.&lt;/p&gt;

&lt;h1 id=&quot;when-should-we-do-feature-scaling&quot;&gt;When should we do feature scaling?&lt;/h1&gt;
&lt;p&gt;There are two cases often mentioned as reasons for scaling: (1) to prevent feature bias when using distance-based models, and (2) to improve the performance of gradient descent [1][8][9].&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Distance-based models
Models that use distances between data points like KNN, K-means, PCA, and SVM should do normalization. If there is a feature having a wide range of values, the distance will be dominated by this feature. So we make features be on a similar scale that each feature contributes approximately proportionately to the final distance. You can witness its benefit in the example made by Brownlee; it demonstarates that the KMM modeling brings different accuracy results with a not-scaled (that is, raw) and scaled data.&lt;/li&gt;
  &lt;li&gt;Gradient descent-based optimization algorithms
Scaling brings benefit to models like linear regression that use gradient descent for optimization, since gradient descent converges much faster.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;
&lt;p&gt;How can we make features take similar ranges of values? While there are many others, you may run into Z-score normalization and Min-Max normalization most often when you search for feature scaling methods. I cautiously anticipate that they are most often used in practices. Let’s first look at the brief description of each of them and then comparisons focusing on when to use which one.&lt;/p&gt;

&lt;h2 id=&quot;z-score-normalization-often-called-standardization&quot;&gt;Z-score normalization (often called Standardization)&lt;/h2&gt;
&lt;p&gt;This rescales the ranges of features so that they have distribution with 0 mean value and 1 standard deviation value. The formula is:&lt;/p&gt;

\[x&apos; = \frac{x-\bar{x}}{\sigma}\]

&lt;h2 id=&quot;min-max-normalization&quot;&gt;Min-Max normalization&lt;/h2&gt;
&lt;p&gt;This rescales values to a fixed range, usually [0, 1] or [-1, 1]. The formula for a min-max of [0, 1] is:&lt;/p&gt;

\[x&apos; = \frac{x-min(x)}{max(x) - min(x))}\]

&lt;p&gt;The formula to rescale to a range of [a, b] is:&lt;/p&gt;

\[x&apos; = a + \frac{(x-min(x))(b-a)}{max(x) - min(x))}\]

&lt;h2 id=&quot;z-score-normalization-vs-min-max-normalization&quot;&gt;Z-score normalization vs. Min-Max normalization&lt;/h2&gt;
&lt;p&gt;(1) Min-Max normalization is affected by outliers much, and Standardization is much less affected by outliers. I found a nice demonstration on this aspect in the Codecademy website [10]. Here is my replication and summary of it. First, suppose a dataset of 100 houses with two variables, the number of rooms and the total years of the house. The number of rooms ranges between 1 and 20. The years of 99 houses range between 0 and 40, and one is 100 years old – an outlier. The below compares results of scaling:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-09-09-feature-scaling-min-max-z-score/houses_raw.png&quot; alt=&quot;Raw Data&quot; /&gt;
&lt;img src=&quot;/blog-site/assets/img/2022-09-09-feature-scaling-min-max-z-score/houses_minmax.png&quot; alt=&quot;Min-Max Normalization&quot; /&gt;
&lt;img src=&quot;/blog-site/assets/img/2022-09-09-feature-scaling-min-max-z-score/houses_zscore.png&quot; alt=&quot;Z-Score Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With min-max normalization, the 99 values of the age variable are located between 0 and 0.4, while all the values of the number of rooms are spread between 0 and 1. With z-score normalization, most (99 or 100) values are located between about -1.5 to 1.5 or -2 to 2, which are similiar ranges.&lt;/p&gt;

&lt;p&gt;Note: I ran into a blog post saying that Min-Max normalization is not affected by outliers and Standardization is. I guess that the author means the bounding range of Min-Max normalization results and the unbounding range of Standardization.&lt;/p&gt;

&lt;p&gt;(2) Often, Standardization is recommended to use when the feature distribution is Normal or Gaussian, and Min-Max normalization, when it is not [11]. Min-Max normalization just changes the range of data and Standardization changes it radically, that is, does not transform the underlying distribution structure of the data. I interprete that this is why it is OK to apply Standardization to normally distributed data (since the normal data would end up the normal data), but not recommended to apply it to non-normal data.&lt;/p&gt;

&lt;p&gt;However, it seems there is no hard and fast rules. Here is Brownlee’s suggestion [11]:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Predictive modeling problems can be complex, and it may not be clear how to best scale input data. If in doubt, normalize the input sequence. If you have the resources, explore modeling with the raw data, standardized data, and normalized data and see if there is a beneficial difference in the performance of the resulting model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let’s look at a few other scaling methods.&lt;/p&gt;

&lt;h2 id=&quot;mean-normalization&quot;&gt;Mean normalization&lt;/h2&gt;
&lt;p&gt;This rescales the values to a range of [-1, 1] with 0 average value. That is, it results in a distribution centered at 0, with its minimum and maximum values within the range of -1 to 1.&lt;/p&gt;

\[x&apos; = \frac{x-\bar{x}}{max(x) - min(x)}\]

&lt;h2 id=&quot;robust-scalar-scaling-to-median-and-quantiles&quot;&gt;Robust Scalar (Scaling to median and quantiles)&lt;/h2&gt;
&lt;p&gt;This uses the median and quantiles. It is less affected by outliers than the Min-Max.&lt;/p&gt;

\[x&apos; = \frac{x-x_{median}}{IQR}\]

&lt;h2 id=&quot;scaling-to-unit-length&quot;&gt;Scaling to unit length&lt;/h2&gt;
&lt;p&gt;It rescales the components of a feature vector so that the length of the scaled vector is 1. One of two norm types is used; that is, divide each compoment by either the Manhattan distance (l1 norm) or the Euclidean distance (l2 norm) of the vector.&lt;/p&gt;

&lt;h1 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h1&gt;
&lt;p&gt;I hope that I perform deeper survey and enough experiments to figure out kind of my own guidance, if any, for choosing a suitable scaling method like Hale is doing[12], although it seems not achievable to definite rules[13]. However, knowing on the underlying math behind models\scaling methods will be definitly helpful in saving time and understanding results.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;https://en.wikipedia.org/wiki/Feature_scaling&lt;/li&gt;
  &lt;li&gt;https://developers.google.com/machine-learning/data-prep/transform/normalization&lt;/li&gt;
  &lt;li&gt;https://www.geeksforgeeks.org/ml-feature-scaling-part-2/&lt;/li&gt;
  &lt;li&gt;https://medium.com/@isalindgren313/transformations-scaling-and-normalization-420b2be12300&lt;/li&gt;
  &lt;li&gt;https://www.statology.org/standardization-vs-normalization/&lt;/li&gt;
  &lt;li&gt;https://www.geeksforgeeks.org/normalization-vs-standardization/&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf&lt;/li&gt;
  &lt;li&gt;https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/&lt;/li&gt;
  &lt;li&gt;https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e&lt;/li&gt;
  &lt;li&gt;https://www.codecademy.com/article/normalization&lt;/li&gt;
  &lt;li&gt;https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02&lt;/li&gt;
  &lt;li&gt;https://www.kdnuggets.com/2019/04/normalization-vs-standardization-quantitative-analysis.html/2&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 09 Sep 2022 00:00:00 -0700</pubDate>
      </item>
    
      <item>
        <title>Sample size formulas</title>
        <link>/blog-site/2022/07/25/sample-size-formulas.html</link>
        <guid isPermaLink="true">/blog-site/2022/07/25/sample-size-formulas.html</guid>
        <description>&lt;p&gt;Although online tools are available to obtain a sample size,  I make a note of formulas for sample size calculation. Being aware of them may help you use online calculators more confidently. It seems that formulas come in different flavors. However, there are more commonly used ones. I will list them up, after check the fundamentals of sample size determination.&lt;/p&gt;

&lt;h1 id=&quot;factors-to-consider-in-sample-size-determination&quot;&gt;Factors to consider in sample size determination&lt;/h1&gt;

&lt;p&gt;Before formulas, let’s look through what affect the sample size:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;It depends on the &lt;strong&gt;size of difference&lt;/strong&gt; that you would like to detect. If differences are smaller, a larger sample size is required. That is, you may not be able to detect a small difference (e.g., cannot reject the null hypothesis), if a sample size is not large enough for it. In other words, larger differences can be caught with smaller sample sizes.&lt;/li&gt;
  &lt;li&gt;It depends on the &lt;strong&gt;standard deviation&lt;/strong&gt;. The smaller the standard deviation is, the smaller the sample size it.&lt;/li&gt;
  &lt;li&gt;It depends on the &lt;strong&gt;size of Type 1 error&lt;/strong&gt;. If you are strict on it and thus set it smaller, you need a larger sample size.&lt;/li&gt;
  &lt;li&gt;It depends on the &lt;strong&gt;size of Type 2 error&lt;/strong&gt;. If you set it smaller, a larger sample size is required. In other words,  you are likely to fail to reject the null hypothesis, if your sample size is not large enough for your Type 2 error level.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In practices, the third and forth factor seems easy to determine. I may follow kind of conventions: e.g., 0.05 or 0.01 for Type 1 error, and 0.1 or 0.2 for Type 2 error. But, the difference and standard deviation are tricky. With regard to the difference, we need to think of what meaningful difference is for a given situation. For instance, “is it worthwhile conducting a study to see if there is 0.1% increase in conversion rate?” With regard to estimation of the standard deviation, it is suggested to refer to existing work, or conduct pilot test, if feasible, and with caution [1][2].&lt;/p&gt;

&lt;h1 id=&quot;for-the-mean&quot;&gt;For the mean&lt;/h1&gt;

&lt;p&gt;Formulas for sample size calculation have different detail according to study designs. We will first look at formulas for the mean to give you a sense of how they are formed fundamentally.&lt;/p&gt;

&lt;h2 id=&quot;finding-a-confidence-interval&quot;&gt;Finding a confidence interval&lt;/h2&gt;
&lt;p&gt;Suppose you target a margin of error that is calculated as follow:&lt;/p&gt;

\[E = Z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}\]

&lt;p&gt;By solving this for n using algebra, you get the below formula that results in the sample size needed for the target margin of error:&lt;/p&gt;

\[n = \frac{Z_{1-\alpha/2}^2\sigma^2}{E^2}\]

&lt;h2 id=&quot;hypothesis-testing&quot;&gt;Hypothesis testing&lt;/h2&gt;
&lt;p&gt;We consider not only Type 1 error, but also Type 2 error. In general, the formula for power computation involving the both is as follow (for the two-tailed test) [3]:&lt;/p&gt;

\[Z_{power} = \frac{\delta}{SE} - Z_{1-\alpha/2}\]

&lt;h3 id=&quot;one-sample-test&quot;&gt;One sample test&lt;/h3&gt;

&lt;p&gt;For the one sample t-test where \(\delta = \lvert\bar{x} - \mu\rvert\), we plug the below and solve for n with some algebra:&lt;/p&gt;

\[SE = \frac{\sigma}{\sqrt{n}}\]

&lt;p&gt;The derived formula is:&lt;/p&gt;

\[n =\frac{(Z_{1-\alpha/2} + Z_{power})^2\sigma^2}{\delta^2}\]

&lt;h3 id=&quot;two-sample-test&quot;&gt;Two sample test&lt;/h3&gt;

&lt;p&gt;For the two sample test where \(\delta = \lvert\bar{x_1} - \bar{x_2}\rvert\), we plug the below:&lt;/p&gt;

\[SE = \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{rn_2}},\newline
\text{where r is ratio of group 2 to group 1 size}\]

&lt;p&gt;With some algebra, we can obtain the following [4]:&lt;/p&gt;

\[n_1 = \frac{(r+1)}{r}\frac{\sigma^2(Z_{power}+Z_{\alpha/2})^2}{\delta^2}\]

&lt;p&gt;If r=1 (equal groups), then&lt;/p&gt;

\[n_1 = \frac{2\sigma^2(Z_{power}+Z_{\alpha/2})^2}{\delta^2}\]

&lt;h3 id=&quot;paired-test&quot;&gt;Paired test&lt;/h3&gt;

&lt;p&gt;The formula for the paired test is [4]:&lt;/p&gt;

\[n = \frac{\sigma_d^2(Z_{power}+Z_{\alpha/2})^2}{\delta^2},\newline
\text{where}~\sigma_d^2~\text{is the standard deviation of the within-pair difference.}\]

&lt;h1 id=&quot;for-the-proportion&quot;&gt;For the proportion&lt;/h1&gt;
&lt;p&gt;For a study to get a confidence interval,&lt;/p&gt;

\[n = \frac{Z_{1-\alpha/2}^2p(1-p)}{E^2}\]

&lt;p&gt;For one sample test,&lt;/p&gt;

\[n =\frac{(Z_{1-\alpha/2} + Z_{power})^2p(1-p)}{\delta^2}\]

&lt;p&gt;For two sample test [4],&lt;/p&gt;

\[n_1 = \frac{(r+1)}{r}\frac{(Z_{power}+Z_{\alpha/2})^2\{p_1(1-p_1)+p_2(1-p_2)\}}{\delta^2}\]

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;All the above with regard to the hypothesis testing assumes two-tailed test; if we do one-tailed test, \(Z_{1-\alpha}\) used, intead of \(Z_{1-\alpha/2}\).&lt;/li&gt;
  &lt;li&gt;If the standard deviation of the population is unknown and the sample size is small, the formulation depends on the t distribution. It causes a problem because critical values of the t distribution depneds on degrees of freedom, which depends on the sample size which we are trying to estimate. &lt;a href=&quot;https://www.itl.nist.gov/div898/handbook/prc/section2/prc222.htm&quot;&gt;This document describes how to manage this issue.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;adjustments&quot;&gt;Adjustments&lt;/h1&gt;
&lt;p&gt;While we have looked around a collection of formulas and may feel quite ready, we need to be aware of situations where those formulas will not work well, and some adjustment is necessary [5].&lt;/p&gt;

&lt;p&gt;When we draw a sample from a finite small population, it is suggested to apply the finite population correction factor. The derived formula is as follows [6]:&lt;/p&gt;

\[n_{finit\_pop} = \frac{n}{1+(\frac{n-1}{N})} = \frac{nN}{n+(N-1)}\newline
\text{where N is the population size.}\]

&lt;h1 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h1&gt;

&lt;p&gt;We have looked around formulas for sample size calculation that are most commonly mentioned. Athough a bit of difference exists in detail according to study designs, they share the same fundamental that was reviewed at the beginning of this post. Although the knowledge covered here may be a small fraction of the whole body of work on the sample size, it should be fairly enough for confident use of existing tools.&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference:&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Whitehead AL, Julious SA, Cooper CL, Campbell MJ. Estimating the sample size for a pilot randomised trial to minimise the overall trial sample size for the external pilot and main trial for a continuous outcome variable. Stat Methods Med Res. 2016 Jun; 25(3):1057-73. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4876429/&quot;&gt;(link)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bell ML, Whitehead AL, Julious SA. Guidance for using pilot studies to inform the design of intervention trials with continuous outcomes. Clin Epidemiol. 2018;10:153-157. &lt;a href=&quot;https://www.dovepress.com/guidance-for-using-pilot-studies-to-inform-the-design-of-intervention--peer-reviewed-fulltext-article-CLEP&quot;&gt;(link)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;WISE, Claremont Graduate University. Power Calculations for One-Sample Z Test in Power Tutorial. &lt;a href=&quot;https://wise1.cgu.edu/power/computing.asp&quot;&gt;(link)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Kristin L. Sainani. Lecture 11 of course hrp259. &lt;a href=&quot;https://web.stanford.edu/~kcobb/hrp259/&quot;&gt;(link)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Monitoring &amp;amp; Evaluation. Sample Size Calculation. &lt;a href=&quot;http://monitoringevaluation.weebly.com/sample-size-adjustments.html&quot;&gt;(link)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Cochran, William G. Sampling techniques. John Wiley &amp;amp; Sons, 1977.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 25 Jul 2022 00:00:00 -0700</pubDate>
      </item>
    
      <item>
        <title>Hacker Statistics, simulation of data acquisition</title>
        <link>/blog-site/2022/07/15/hacker-statistics-simulation-of-data-acquisition.html</link>
        <guid isPermaLink="true">/blog-site/2022/07/15/hacker-statistics-simulation-of-data-acquisition.html</guid>
        <description>&lt;p&gt;I will share the concept and applications of “hacker statistics”, an approach for statistical inference. I first try to identify its definition, and then present several examples that apply it. It is considered a quite versitile tool, although there remain a number of questions to study further.&lt;/p&gt;

&lt;p&gt;What is hacker statistics? My online search did not bring much information on it, and it looks like  there is no established definition. But heavily influenced by Justin Bois’s tutorial[1], I understand it as “simulating data acquisition under an assumption to find the probability of an aspect of interest.”  So, next, I will present examples found in Justin Bois’s tutorial[1].&lt;/p&gt;

&lt;h1 id=&quot;examples&quot;&gt;Examples&lt;/h1&gt;

&lt;h2 id=&quot;what-is-the-probability-of-getting-4-heads-when-a-coin-with-probability-05-of-heads-is-tossed-4-times&quot;&gt;What is the probability of getting 4 heads when a coin with probability 0.5 of heads is tossed 4 times?&lt;/h2&gt;
&lt;p&gt;To get an answer to it following the spirit of hacker stats, we do a lot of 4-flip trial recording the number of heads each trial to get an answer, rather than rely on any formula. But, instead of manually doing it, we can realize a lot of coin flipping using a random number generator and for loop. The below code generates 10000 repeats of the 4-flip trial, counting the number of heads. We get the number of  trials with  4 heads, and finally the probability of getting 4 heads in 4 tosses.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Code from [1]
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_all_heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Count number of 4-head trials
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 10000 repeats of the four flip trials
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# If a given trial has four heads, increase the count
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;n_all_heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        
&lt;span class=&quot;n&quot;&gt;n_all_heads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# What is the probability of getting all four heads?
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;what-is-the-confidence-interval-of-a-sample&quot;&gt;What is the confidence interval of a sample?&lt;/h2&gt;
&lt;p&gt;Suppose we have an observed data set of the rainfall amount in millimeters ([122.1,  69.8,  29.6, …,  65.8,  58.2, 130.4]). To get a confidence interval of the mean using hacker stats, we need to be able to simulate repeated measurements, and here comes  ‘bootstrapping’, sampling a dataset with replacement.&lt;/p&gt;

&lt;p&gt;The below code implements &lt;a href=&quot;https://en.wikipedia.org/wiki/Bootstrapping_(statistics)&quot;&gt;bootstrapping&lt;/a&gt; using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.random.choice()&lt;/code&gt; in a for loop. It generates 10000 samples, and calculates the mean of each of them. That is, a sampling distribution of the mean is made, and a confidence interval is found by looking at 2.5 and 97.5th percentiles.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;bs_replicates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Generate a bootstrap sample
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;bs_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rainfall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rainfall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Resampling engine
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;bs_replicates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;c1&quot;&gt;# 95 percent confidence interval
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf_int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;percentile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs_replicates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;97.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;is-the-amount-of-rainfall-in-november-and-june-different&quot;&gt;Is the amount of rainfall in November and June different?&lt;/h2&gt;
&lt;p&gt;Suppose we have a dataset, a record of the amount of rainfall during November and June. To see if these months have different amount of rainfall indeed, we set the following hypothesis to test:&lt;/p&gt;

\[H_0: \mu_{nov} = \mu_{jun}\newline
H_1: \mu_{nov} \not = \mu_{jun}\]

&lt;p&gt;To do the test with hacker stats, we need to generate many sets of data considering the null hypothesis is true. As a simulation method, there is permutation resampling: merge the two data sets, and iteratively divide the all values into two groups of the same sizes as the two data sets, in every possible way (every possible permutation)[2]. For each iteration, the difference of the two groups’ means is calculated, and we finally check where the observed difference locates in the distribution of these simulated differences.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;diff_observed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rain_june&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rain_november&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;perm_replicates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 10000 iterations
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Generate a permutation sample
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;perm_sample_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;perm_sample_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;permutation_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rain_june&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rain_november&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Compute the test statistic
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;perm_replicates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;perm_sample_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;perm_sample_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot the histogram
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;perm_replicates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;density&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Mean difference&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;PDF&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Plot vertical lines at the absolute of the observed difference and negative to it
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff_observed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff_observed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-07-15-hacker-statistics-simulation-of-data-acquisition/rainfall_nov_jun.png&quot; alt=&quot;Histogram of simulated differences -- confidence interval&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;empirical_diff_means&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rain_november&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rain_june&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;perm_replicates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;empirical_diff_means&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;perm_replicates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For a p-value, we get the proportion of the distribution where the absolute difference was greater than the absolute value of the observed difference (as the example is the two-tailed test).&lt;/p&gt;

&lt;p&gt;We have looked around the examples that apply the hacker stats technique for different questions, and I believe that you got an idea on what hacker stats is.&lt;/p&gt;

&lt;p&gt;To the best of my knowledge, there are three methods that realize hacker stats [1][3][5]:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Use of an existing model&lt;/li&gt;
  &lt;li&gt;Permutation sampling&lt;/li&gt;
  &lt;li&gt;Bootstrapping&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given them, I wondered about how different results of them would be, when applied to the sample problem. Adapting materials that were found in Bois’ tutorial[1] and Udacity[5], I performed two sample z-test of proportions using each of the three methods.&lt;/p&gt;

&lt;h1 id=&quot;two-sample-z-test-of-proportions-eg-ab-testing-of-click-through-rate&quot;&gt;Two sample z-test of proportions (e.g., A/B testing of click-through rate)&lt;/h1&gt;
&lt;p&gt;Suppose we have to determine whether the new banner design leads to a better click through rate (CTR) than the old one, and we have the below dataset that tells her group and whether she clicked or not, for each unique user (6328 rows × 2 columns).&lt;/p&gt;

&lt;table style=&quot;border: 1; height: 280px;&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group&lt;/th&gt;
      &lt;th&gt;clicked&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;5640&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;376&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4247&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2746&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;651&lt;/th&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5157&lt;/th&gt;
      &lt;td&gt;experiment&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4490&lt;/th&gt;
      &lt;td&gt;experiment&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7745&lt;/th&gt;
      &lt;td&gt;experiment&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;942&lt;/th&gt;
      &lt;td&gt;experiment&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5255&lt;/th&gt;
      &lt;td&gt;experiment&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A hypothesis has been made like this:&lt;/p&gt;

\[H_0: p_{exp} - p_{ctl} \leq 0\newline
H_1: p_{exp} - p_{ctl} &amp;gt; 0\]

&lt;h2 id=&quot;permutation-sampling&quot;&gt;Permutation sampling&lt;/h2&gt;
&lt;p&gt;First, I present use of permutation sampling, borrowing code from [1]. The below generates 10000 CTR differences between 10000 resampled pairs of the control and experiment groups.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# ctl_df, a dataframe made by extracting control group records from &apos;both_df&apos;
# exp_df, experiment group records
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ctl_ctr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctl_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;clicked&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctl_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;exp_ctr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;clicked&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;obs_diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_ctr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctl_ctr&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ctl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctl_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;clicked&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;clicked&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;diffs_pm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Concatenate the data sets: data
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Permute the concatenated array: permuted_data
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;permuted_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Split the permuted array into two: perm_sample_1, perm_sample_2
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ctl_perm_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;permuted_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;exp_perm_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;permuted_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;diff_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_perm_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_perm_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; \
                &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctl_perm_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctl_perm_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Compute the test statistic
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;diffs_pm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff_sample&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diffs_pm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs_diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diffs_pm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;p-value =&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the histogram of the CTR differences (test statistics acquired with the resampling), with a line indicating the observed difference (p-value is 0.0027):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-07-15-hacker-statistics-simulation-of-data-acquisition/ab_test_permutation_histogram.png&quot; alt=&quot;Histogram of simulated differences -- ab test with permutation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can get a p-value by dividing the number of differences larger than the observed difference by the total number of values.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diffs_pm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs_diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diffs_pm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;p-value =&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;bootstrapping&quot;&gt;Bootstrapping&lt;/h2&gt;
&lt;p&gt;Second, I test the hypothesis by generating samples using the bootstrap method.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 1) Compute the observed difference
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs_diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_ctr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctl_ctr&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 2) Simulate the sampling distribution for the difference in proportions
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diffs_bt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Pick from the whole data with replacement
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;b_samp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;both_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;both_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;ctl_samp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_samp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;group == &quot;control&quot;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;exp_samp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_samp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;group == &quot;experiment&quot;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;ctl_samp_ctr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctl_samp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;clicked&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctl_samp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;exp_samp_ctr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_samp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;clicked&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_samp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
    &lt;span class=&quot;n&quot;&gt;diffs_bt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_samp_ctr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctl_samp_ctr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, using the above sampling, a distribution under the null hypothesis is simulated, by creating a random normal distribution centered at 0, with the same spread and size.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;diffs_arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diffs_bt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;null_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diffs_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diffs_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;null_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axvline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs_diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To get a p-value:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;null_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obs_diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the histogram of the simulated CTR rate differences (p value is 0.0039):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-07-15-hacker-statistics-simulation-of-data-acquisition/ab_test_bootstrap_histogram.png&quot; alt=&quot;Histogram of simulated differences -- ab test with bootstrap&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;use-of-binomial-distribution&quot;&gt;Use of binomial distribution&lt;/h2&gt;
&lt;p&gt;As shown below, sample acquistion from a binonimal distribution can be simulated using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.random.choice()&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Calculate a click-through-rate under the null hypothesis
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_null&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;both_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;clicked&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;both_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;diffs_bd_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;new_page_converted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;old_page_converted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ctl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p_null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;diffs_bd_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_page_converted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_page_converted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the histogram of the results (p value is 0.0049):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-07-15-hacker-statistics-simulation-of-data-acquisition/ab_test_binomial_choice_histogram.png&quot; alt=&quot;Histogram of simulated differences -- ab test with binomial distribution np.random.choice&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Or, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.random.binomial()&lt;/code&gt; can be used as below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;new_simulation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_exp&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;old_simulation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_ctl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_ctl&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;diffs_bd_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_simulation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_simulation&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the histogram of the results (p value is 0.0045):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-07-15-hacker-statistics-simulation-of-data-acquisition/ab_test_binomial_binomial_histogram.png&quot; alt=&quot;Histogram of simulated differences -- ab test with binomial distribution np.random.binomial&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In addition to the use of hacker stats, I calculated a p-value using the parametric method (statsmodels.stats.proportion.proportions_ztest()).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;statsmodels.stats.proportion&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportions_ztest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportion_confint&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;successes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;928&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;932&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nobs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_ctl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;z_stat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportions_ztest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;successes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alternative&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;larger&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower_con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower_treat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;upper_con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upper_treat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportion_confint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;successes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nobs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;z statistic: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z_stat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;p-value: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;notebook_output&quot;&gt;
&lt;pre&gt;
z statistic: 2.62
p-value: 0.00442
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;You can find all the code in my &lt;a href=&quot;https://github.com/jisooleeworks/blog-code/blob/main/hacker_stats_AB_testing-upload.ipynb&quot;&gt;Github page&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let’s compare all the p-values:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Permutation method: 0.0027&lt;/li&gt;
  &lt;li&gt;Bootstrapping: 0.0039&lt;/li&gt;
  &lt;li&gt;Use of binomial distribution with np.random.choice(): 0.0045&lt;/li&gt;
  &lt;li&gt;Use of binomial distribution with np.random.binomial(): 0.0049&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And, with the parametric method, 0.00442&lt;/p&gt;

&lt;p&gt;Since we generated samples using random number generators, the results with the hackter stats methods are not fixed ones. I ran the code a few times, and compared them as below:&lt;/p&gt;
&lt;pre style=&quot;font-size:0.8em; padding-left:2em; padding-right:2em;&quot;&gt;
perm 0.0027 &amp;lt; boot 0.0039 &amp;lt; para 0.00442 &amp;lt; bi 0.0045 &amp;lt; choice 0.0049
choice 0.0026 &amp;lt; perm 0.003 &amp;lt; boot 0.0039 &amp;lt; para 0.00442 &amp;lt; bi 0.0054
perm 0.0013 &amp;lt; choice 0.0033 &amp;lt; boot 0.0037 &amp;lt; bi 0.0039 &amp;lt; para 0.00442
perm 0.0025  &amp;lt; para 0.00442 &amp;lt; choice 0.0046 &amp;lt; boot 0.0051 &amp;lt; bi 0.0053
&lt;/pre&gt;

&lt;p&gt;Given the results, the below questions came to my mind:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The p values are somewhat different each other. Which should we pick as a final result? Which one is the most trustable?&lt;/li&gt;
  &lt;li&gt;In my observation, the permutation sampling results in a smaller p-value than the bootstrapping. Will it be always true no matter what data are tested?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Justin Bois. Datacamp. &lt;a href=&quot;https://www.datacamp.com/courses/statistical-thinking-in-python-part-1&quot;&gt;Statistical thinking (1)&lt;/a&gt; and &lt;a href=&quot;https://www.datacamp.com/courses/statistical-thinking-in-python-part-2&quot;&gt;Statistical thinking (2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Wikipedia. &lt;a href=&quot;https://en.wikipedia.org/wiki/Permutation_test&quot;&gt;Permutation test&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yevgeniy (Gene) Mishchenko. &lt;a href=&quot;https://medium.com/towards-data-science/bootstrapping-vs-permutation-testing-a30237795970&quot;&gt;Bootstrapping vs. Permutation Testing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Matthew E. Parker. &lt;a href=&quot;https://medium.com/@mattheweparker/common-machine-learning-resampling-methods-like-bootstrapping-and-permutation-testing-attempt-to-ddc4fbbda391&quot;&gt;Resampling (Bootstrapping &amp;amp; Permutation Testing)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Udacity, &lt;a href=&quot;https://www.udacity.com/course/data-analyst-nanodegree--nd002&quot;&gt;Data Analyst Nanodegree&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Jake Vanderplas. &lt;a href=&quot;https://www.youtube.com/watch?v=Iq9DzN6mvYA&quot;&gt;Statistics for Hackers - PyCon 2016&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 15 Jul 2022 00:00:00 -0700</pubDate>
      </item>
    
      <item>
        <title>What are associated with profitable movies?</title>
        <link>/blog-site/2022/06/01/what-are-associated-with-profitable-movies.html</link>
        <guid isPermaLink="true">/blog-site/2022/06/01/what-are-associated-with-profitable-movies.html</guid>
        <description>&lt;p&gt;Using a &lt;a href=&quot;https://www.kaggle.com/datasets/juzershakir/tmdb-movies-dataset&quot;&gt;TMDb movie data set&lt;/a&gt; containing properties of movies released between 1960 and 2015, I explored what are related to financial success of movies, with visualizations.&lt;/p&gt;

&lt;p&gt;In assessing financial success, I used &lt;strong&gt;Return on Investment (ROI)&lt;/strong&gt;, a measure commonly used in describing profitability. It is calculated by: ROI = (gross revenue - cost)/cost. Of the properties, I was most interested in &lt;strong&gt;budget, genres, and vote scores&lt;/strong&gt;. Specifically, I investigated:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Did high budget movies achieve high ROI?&lt;/li&gt;
  &lt;li&gt;Did specific genres lead to high ROI?&lt;/li&gt;
  &lt;li&gt;Did high score movies achieve high ROI?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The work process was:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#ch1&quot;&gt;Data overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ch2&quot;&gt;Data wrangling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ch3&quot;&gt;Univariate exploration&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ch4&quot;&gt;Q1: Did high budget movies achieve high ROI?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ch5&quot;&gt;Q2: Did specific genres lead to high ROI?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ch6&quot;&gt;Q3: Did high score movies achieve high ROI?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ch7&quot;&gt;Conclusions (Summary, Limitations)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will walk through the process here. You can see code and all details &lt;a href=&quot;https://github.com/jisooleeworks/blog-code/blob/main/tmdb_post.ipynb&quot;&gt;in my GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;data-overview&quot;&gt;Data overview&lt;a class=&quot;anchor&quot; id=&quot;ch1&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;After loading the data set, I looked at a couple of fundamental aspects.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;imdb_id&lt;/th&gt;
      &lt;th&gt;popularity&lt;/th&gt;
      &lt;th&gt;budget&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;original_title&lt;/th&gt;
      &lt;th&gt;cast&lt;/th&gt;
      &lt;th&gt;homepage&lt;/th&gt;
      &lt;th&gt;director&lt;/th&gt;
      &lt;th&gt;tagline&lt;/th&gt;
      &lt;th&gt;keywords&lt;/th&gt;
      &lt;th&gt;overview&lt;/th&gt;
      &lt;th&gt;runtime&lt;/th&gt;
      &lt;th&gt;genres&lt;/th&gt;
      &lt;th&gt;production_companies&lt;/th&gt;
      &lt;th&gt;release_date&lt;/th&gt;
      &lt;th&gt;vote_count&lt;/th&gt;
      &lt;th&gt;vote_average&lt;/th&gt;
      &lt;th&gt;release_year&lt;/th&gt;
      &lt;th&gt;budget_adj&lt;/th&gt;
      &lt;th&gt;revenue_adj&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;135397&lt;/td&gt;
      &lt;td&gt;tt0369610&lt;/td&gt;
      &lt;td&gt;32.985763&lt;/td&gt;
      &lt;td&gt;150000000&lt;/td&gt;
      &lt;td&gt;1513528810&lt;/td&gt;
      &lt;td&gt;Jurassic World&lt;/td&gt;
      &lt;td&gt;Chris Pratt|Bryce Dallas Howard|Irrfan Khan|Vi...&lt;/td&gt;
      &lt;td&gt;http://www.jurassicworld.com/&lt;/td&gt;
      &lt;td&gt;Colin Trevorrow&lt;/td&gt;
      &lt;td&gt;The park is open.&lt;/td&gt;
      &lt;td&gt;monster|dna|tyrannosaurus rex|velociraptor|island&lt;/td&gt;
      &lt;td&gt;Twenty-two years after the events of Jurassic ...&lt;/td&gt;
      &lt;td&gt;124&lt;/td&gt;
      &lt;td&gt;Action|Adventure|Science Fiction|Thriller&lt;/td&gt;
      &lt;td&gt;Universal Studios|Amblin Entertainment|Legenda...&lt;/td&gt;
      &lt;td&gt;6/9/15&lt;/td&gt;
      &lt;td&gt;5562&lt;/td&gt;
      &lt;td&gt;6.5&lt;/td&gt;
      &lt;td&gt;2015&lt;/td&gt;
      &lt;td&gt;1.379999e+08&lt;/td&gt;
      &lt;td&gt;1.392446e+09&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;76341&lt;/td&gt;
      &lt;td&gt;tt1392190&lt;/td&gt;
      &lt;td&gt;28.419936&lt;/td&gt;
      &lt;td&gt;150000000&lt;/td&gt;
      &lt;td&gt;378436354&lt;/td&gt;
      &lt;td&gt;Mad Max: Fury Road&lt;/td&gt;
      &lt;td&gt;Tom Hardy|Charlize Theron|Hugh Keays-Byrne|Nic...&lt;/td&gt;
      &lt;td&gt;http://www.madmaxmovie.com/&lt;/td&gt;
      &lt;td&gt;George Miller&lt;/td&gt;
      &lt;td&gt;What a Lovely Day.&lt;/td&gt;
      &lt;td&gt;future|chase|post-apocalyptic|dystopia|australia&lt;/td&gt;
      &lt;td&gt;An apocalyptic story set in the furthest reach...&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;Action|Adventure|Science Fiction|Thriller&lt;/td&gt;
      &lt;td&gt;Village Roadshow Pictures|Kennedy Miller Produ...&lt;/td&gt;
      &lt;td&gt;5/13/15&lt;/td&gt;
      &lt;td&gt;6185&lt;/td&gt;
      &lt;td&gt;7.1&lt;/td&gt;
      &lt;td&gt;2015&lt;/td&gt;
      &lt;td&gt;1.379999e+08&lt;/td&gt;
      &lt;td&gt;3.481613e+08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;262500&lt;/td&gt;
      &lt;td&gt;tt2908446&lt;/td&gt;
      &lt;td&gt;13.112507&lt;/td&gt;
      &lt;td&gt;110000000&lt;/td&gt;
      &lt;td&gt;295238201&lt;/td&gt;
      &lt;td&gt;Insurgent&lt;/td&gt;
      &lt;td&gt;Shailene Woodley|Theo James|Kate Winslet|Ansel...&lt;/td&gt;
      &lt;td&gt;http://www.thedivergentseries.movie/#insurgent&lt;/td&gt;
      &lt;td&gt;Robert Schwentke&lt;/td&gt;
      &lt;td&gt;One Choice Can Destroy You&lt;/td&gt;
      &lt;td&gt;based on novel|revolution|dystopia|sequel|dyst...&lt;/td&gt;
      &lt;td&gt;Beatrice Prior must confront her inner demons ...&lt;/td&gt;
      &lt;td&gt;119&lt;/td&gt;
      &lt;td&gt;Adventure|Science Fiction|Thriller&lt;/td&gt;
      &lt;td&gt;Summit Entertainment|Mandeville Films|Red Wago...&lt;/td&gt;
      &lt;td&gt;3/18/15&lt;/td&gt;
      &lt;td&gt;2480&lt;/td&gt;
      &lt;td&gt;6.3&lt;/td&gt;
      &lt;td&gt;2015&lt;/td&gt;
      &lt;td&gt;1.012000e+08&lt;/td&gt;
      &lt;td&gt;2.716190e+08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;140607&lt;/td&gt;
      &lt;td&gt;tt2488496&lt;/td&gt;
      &lt;td&gt;11.173104&lt;/td&gt;
      &lt;td&gt;200000000&lt;/td&gt;
      &lt;td&gt;2068178225&lt;/td&gt;
      &lt;td&gt;Star Wars: The Force Awakens&lt;/td&gt;
      &lt;td&gt;Harrison Ford|Mark Hamill|Carrie Fisher|Adam D...&lt;/td&gt;
      &lt;td&gt;http://www.starwars.com/films/star-wars-episod...&lt;/td&gt;
      &lt;td&gt;J.J. Abrams&lt;/td&gt;
      &lt;td&gt;Every generation has a story.&lt;/td&gt;
      &lt;td&gt;android|spaceship|jedi|space opera|3d&lt;/td&gt;
      &lt;td&gt;Thirty years after defeating the Galactic Empi...&lt;/td&gt;
      &lt;td&gt;136&lt;/td&gt;
      &lt;td&gt;Action|Adventure|Science Fiction|Fantasy&lt;/td&gt;
      &lt;td&gt;Lucasfilm|Truenorth Productions|Bad Robot&lt;/td&gt;
      &lt;td&gt;12/15/15&lt;/td&gt;
      &lt;td&gt;5292&lt;/td&gt;
      &lt;td&gt;7.5&lt;/td&gt;
      &lt;td&gt;2015&lt;/td&gt;
      &lt;td&gt;1.839999e+08&lt;/td&gt;
      &lt;td&gt;1.902723e+09&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;168259&lt;/td&gt;
      &lt;td&gt;tt2820852&lt;/td&gt;
      &lt;td&gt;9.335014&lt;/td&gt;
      &lt;td&gt;190000000&lt;/td&gt;
      &lt;td&gt;1506249360&lt;/td&gt;
      &lt;td&gt;Furious 7&lt;/td&gt;
      &lt;td&gt;Vin Diesel|Paul Walker|Jason Statham|Michelle ...&lt;/td&gt;
      &lt;td&gt;http://www.furious7.com/&lt;/td&gt;
      &lt;td&gt;James Wan&lt;/td&gt;
      &lt;td&gt;Vengeance Hits Home&lt;/td&gt;
      &lt;td&gt;car race|speed|revenge|suspense|car&lt;/td&gt;
      &lt;td&gt;Deckard Shaw seeks revenge against Dominic Tor...&lt;/td&gt;
      &lt;td&gt;137&lt;/td&gt;
      &lt;td&gt;Action|Crime|Thriller&lt;/td&gt;
      &lt;td&gt;Universal Pictures|Original Film|Media Rights ...&lt;/td&gt;
      &lt;td&gt;4/1/15&lt;/td&gt;
      &lt;td&gt;2947&lt;/td&gt;
      &lt;td&gt;7.3&lt;/td&gt;
      &lt;td&gt;2015&lt;/td&gt;
      &lt;td&gt;1.747999e+08&lt;/td&gt;
      &lt;td&gt;1.385749e+09&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;notebook_output&quot;&gt;
&lt;pre&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
RangeIndex: 10866 entries, 0 to 10865
Data columns (total 21 columns):
 #   Column                Non-Null Count  Dtype  
---  ------                --------------  -----  
 0   id                    10866 non-null  int64  
 1   imdb_id               10856 non-null  object 
 2   popularity            10866 non-null  float64
 3   budget                10866 non-null  int64  
 4   revenue               10866 non-null  int64  
 5   original_title        10866 non-null  object 
 6   cast                  10790 non-null  object 
 7   homepage              2936 non-null   object 
 8   director              10822 non-null  object 
 9   tagline               8042 non-null   object 
 10  keywords              9373 non-null   object 
 11  overview              10862 non-null  object 
 12  runtime               10866 non-null  int64  
 13  genres                10843 non-null  object 
 14  production_companies  9836 non-null   object 
 15  release_date          10866 non-null  object 
 16  vote_count            10866 non-null  int64  
 17  vote_average          10866 non-null  float64
 18  release_year          10866 non-null  int64  
 19  budget_adj            10866 non-null  float64
 20  revenue_adj           10866 non-null  float64
dtypes: float64(4), int64(6), object(11)
memory usage: 1.7+ MB
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The data set had 10866 records with 21 columns. Some columns, like ‘cast’ and ‘genres’, contained multiple values separated by pipe characters. With regard to budget and revenue, the data set had both original and inflation adjusted value column. I chose to use the inflation adjusted ones.&lt;/p&gt;

&lt;h1 id=&quot;data-wrangling&quot;&gt;Data wrangling&lt;a class=&quot;anchor&quot; id=&quot;ch2&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;In this step, I dealt with:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Duplicates&lt;/li&gt;
  &lt;li&gt;Missing values&lt;/li&gt;
  &lt;li&gt;Unused columns&lt;/li&gt;
  &lt;li&gt;Incorrect values&lt;/li&gt;
  &lt;li&gt;Extraction of a variable of interest (i.e., ROI)&lt;/li&gt;
  &lt;li&gt;Data type&lt;/li&gt;
  &lt;li&gt;Outliers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;duplicates&quot;&gt;Duplicates&lt;/h2&gt;
&lt;p&gt;First of all, I examined if there were any duplicate records by checking for (1) duplicate id values, and (2) records with the same ‘title’, ‘director’, ‘release_year’, and ‘runtime’.&lt;/p&gt;

&lt;h3 id=&quot;based-on-id&quot;&gt;Based on ‘id’&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;duplicated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;id&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keep&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;table style=&quot;border:1;height:280px;&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;imdb_id&lt;/th&gt;
      &lt;th&gt;popularity&lt;/th&gt;
      &lt;th&gt;budget&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;original_title&lt;/th&gt;
      &lt;th&gt;cast&lt;/th&gt;
      &lt;th&gt;homepage&lt;/th&gt;
      &lt;th&gt;director&lt;/th&gt;
      &lt;th&gt;tagline&lt;/th&gt;
      &lt;th&gt;keywords&lt;/th&gt;
      &lt;th&gt;overview&lt;/th&gt;
      &lt;th&gt;runtime&lt;/th&gt;
      &lt;th&gt;genres&lt;/th&gt;
      &lt;th&gt;production_companies&lt;/th&gt;
      &lt;th&gt;release_date&lt;/th&gt;
      &lt;th&gt;vote_count&lt;/th&gt;
      &lt;th&gt;vote_average&lt;/th&gt;
      &lt;th&gt;release_year&lt;/th&gt;
      &lt;th&gt;budget_adj&lt;/th&gt;
      &lt;th&gt;revenue_adj&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2089&lt;/th&gt;
      &lt;td&gt;42194&lt;/td&gt;
      &lt;td&gt;tt0411951&lt;/td&gt;
      &lt;td&gt;0.59643&lt;/td&gt;
      &lt;td&gt;30000000&lt;/td&gt;
      &lt;td&gt;967000&lt;/td&gt;
      &lt;td&gt;TEKKEN&lt;/td&gt;
      &lt;td&gt;Jon Foo|Kelly Overton|Cary-Hiroyuki Tagawa|Ian...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Dwight H. Little&lt;/td&gt;
      &lt;td&gt;Survival is no game&lt;/td&gt;
      &lt;td&gt;martial arts|dystopia|based on video game|mart...&lt;/td&gt;
      &lt;td&gt;In the year of 2039, after World Wars destroy ...&lt;/td&gt;
      &lt;td&gt;92&lt;/td&gt;
      &lt;td&gt;Crime|Drama|Action|Thriller|Science Fiction&lt;/td&gt;
      &lt;td&gt;Namco|Light Song Films&lt;/td&gt;
      &lt;td&gt;3/20/10&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;30000000.0&lt;/td&gt;
      &lt;td&gt;967000.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2090&lt;/th&gt;
      &lt;td&gt;42194&lt;/td&gt;
      &lt;td&gt;tt0411951&lt;/td&gt;
      &lt;td&gt;0.59643&lt;/td&gt;
      &lt;td&gt;30000000&lt;/td&gt;
      &lt;td&gt;967000&lt;/td&gt;
      &lt;td&gt;TEKKEN&lt;/td&gt;
      &lt;td&gt;Jon Foo|Kelly Overton|Cary-Hiroyuki Tagawa|Ian...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Dwight H. Little&lt;/td&gt;
      &lt;td&gt;Survival is no game&lt;/td&gt;
      &lt;td&gt;martial arts|dystopia|based on video game|mart...&lt;/td&gt;
      &lt;td&gt;In the year of 2039, after World Wars destroy ...&lt;/td&gt;
      &lt;td&gt;92&lt;/td&gt;
      &lt;td&gt;Crime|Drama|Action|Thriller|Science Fiction&lt;/td&gt;
      &lt;td&gt;Namco|Light Song Films&lt;/td&gt;
      &lt;td&gt;3/20/10&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;30000000.0&lt;/td&gt;
      &lt;td&gt;967000.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Two records were detected, identical for all the variables. So, an extra was deleted.&lt;/p&gt;

&lt;h3 id=&quot;based-on-title-director-release_year-and-runtime&quot;&gt;Based on ‘title’, ‘director’, ‘release_year’, and ‘runtime’&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;duplicated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;original_title&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;director&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keep&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;\
    &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;original_title&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;director&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;release_year&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;runtime&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;original_title&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table style=&quot;border:1;height:260px;&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;original_title&lt;/th&gt;
      &lt;th&gt;director&lt;/th&gt;
      &lt;th&gt;release_year&lt;/th&gt;
      &lt;th&gt;runtime&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1400&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Shane Acker&lt;/td&gt;
      &lt;td&gt;2009&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6514&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Shane Acker&lt;/td&gt;
      &lt;td&gt;2005&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4337&lt;/th&gt;
      &lt;td&gt;Bottle Rocket&lt;/td&gt;
      &lt;td&gt;Wes Anderson&lt;/td&gt;
      &lt;td&gt;1994&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8547&lt;/th&gt;
      &lt;td&gt;Bottle Rocket&lt;/td&gt;
      &lt;td&gt;Wes Anderson&lt;/td&gt;
      &lt;td&gt;1996&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4451&lt;/th&gt;
      &lt;td&gt;Frankenweenie&lt;/td&gt;
      &lt;td&gt;Tim Burton&lt;/td&gt;
      &lt;td&gt;2012&lt;/td&gt;
      &lt;td&gt;87&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7943&lt;/th&gt;
      &lt;td&gt;Frankenweenie&lt;/td&gt;
      &lt;td&gt;Tim Burton&lt;/td&gt;
      &lt;td&gt;1984&lt;/td&gt;
      &lt;td&gt;29&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4063&lt;/th&gt;
      &lt;td&gt;Madea&apos;s Family Reunion&lt;/td&gt;
      &lt;td&gt;Tyler Perry&lt;/td&gt;
      &lt;td&gt;2002&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6701&lt;/th&gt;
      &lt;td&gt;Madea&apos;s Family Reunion&lt;/td&gt;
      &lt;td&gt;Tyler Perry&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;110&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5202&lt;/th&gt;
      &lt;td&gt;Saw&lt;/td&gt;
      &lt;td&gt;James Wan&lt;/td&gt;
      &lt;td&gt;2003&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7011&lt;/th&gt;
      &lt;td&gt;Saw&lt;/td&gt;
      &lt;td&gt;James Wan&lt;/td&gt;
      &lt;td&gt;2004&lt;/td&gt;
      &lt;td&gt;103&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The output shows that some movies have another movie with the same title/director/release_year. With online search, I learned that this happens sometimes; e.g., a director makes a ‘mini’ version before the full scale. So I kept all the records.&lt;/p&gt;

&lt;h2 id=&quot;missing-values&quot;&gt;Missing values&lt;/h2&gt;
&lt;p&gt;The ‘genre’ had only 23 nulls, but there were many records with 0 value for the budget and/or revenue. I had to remove all the records that belonged to any of the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;null for ‘genre’&lt;/li&gt;
  &lt;li&gt;0 for ‘budget_adj’&lt;/li&gt;
  &lt;li&gt;0 for ‘revenue_adj’&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;incorrect-values&quot;&gt;Incorrect values&lt;/h2&gt;
&lt;p&gt;As a minimum effort to ensure correctness of budget and revenue values, I inspected data points at both ends (the lowest and highest).&lt;/p&gt;

&lt;h3 id=&quot;top-budget-movie&quot;&gt;Top budget movie&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;budget_adj&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;budget_adj&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table style=&quot;border: 1; height: 68px;&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;budget&lt;/th&gt;
      &lt;th&gt;revenue&lt;/th&gt;
      &lt;th&gt;original_title&lt;/th&gt;
      &lt;th&gt;genres&lt;/th&gt;
      &lt;th&gt;vote_average&lt;/th&gt;
      &lt;th&gt;release_year&lt;/th&gt;
      &lt;th&gt;budget_adj&lt;/th&gt;
      &lt;th&gt;revenue_adj&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2244&lt;/th&gt;
      &lt;td&gt;46528&lt;/td&gt;
      &lt;td&gt;425000000&lt;/td&gt;
      &lt;td&gt;11087569&lt;/td&gt;
      &lt;td&gt;The Warrior&apos;s Way&lt;/td&gt;
      &lt;td&gt;Adventure|Fantasy|Action|Western|Thriller&lt;/td&gt;
      &lt;td&gt;6.4&lt;/td&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;425000000.0&lt;/td&gt;
      &lt;td&gt;11087569.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Although the above says that ‘The Warrior’s Way’ is the highest budget movie by 425 million, online search revealed that the budget of it is 42.5 million. After this correction, the highest budget became $368 millon of ‘Pirates of the Caribbean: On Stranger Tides,’ which is correct.&lt;/p&gt;

&lt;h3 id=&quot;top-revenue-movie&quot;&gt;Top revenue movie&lt;/h3&gt;
&lt;p&gt;‘Avartar’ came out as the highest revenue movie, and online search confirmed it.&lt;/p&gt;

&lt;h3 id=&quot;bottom-budget-movie&quot;&gt;Bottom budget movie&lt;/h3&gt;
&lt;p&gt;I displayed several lowest budget movies, and got puzzled with extremely low values like $1, $50, or ‘$100. I did online search and found that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;‘Primer (2004)’ is considered as the lowest budget movie (https://en.wikipedia.org/wiki/Low-budget_film)&lt;/li&gt;
  &lt;li&gt;Many of the low budget values are wrong; for example, the budget of ‘Lost &amp;amp; Found (1999)’ is $30 million, not $1.3, ‘Joyful Noise (2012)’, $25 millon, not $23, ‘Weekend (2011)’, $0.14 millon, not $7755, etc. 
With the above findings, I inclined to believe that $8081, budget of ‘Primer (2004)’, should be the lowest. I removed movies whose budget was lower than it.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bottom-revenue-movie&quot;&gt;Bottom revenue movie&lt;/h3&gt;
&lt;p&gt;Again, online search made me distrust data points whose revenue was lower than the revenue of ‘Best Man Down’ ($1840.6). They were removed.&lt;/p&gt;

&lt;h2 id=&quot;data-type&quot;&gt;Data type&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;genres&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;genres&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;|&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Values of ‘genres’ transformed from string to list, e.g., Action|Adventure|Science Fiction –&amp;gt; [Action, Adventure, Science Fiction], for convenience in later analysis.&lt;/p&gt;

&lt;h2 id=&quot;outliers&quot;&gt;Outliers&lt;/h2&gt;
&lt;p&gt;Fundamentally, I identified records that might inhibit finding a general direction by looking into each variable, and removed them.&lt;/p&gt;

&lt;h3 id=&quot;budget&quot;&gt;Budget&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/ol_budget.png&quot; alt=&quot;Boxplot of budget&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Several records on the top stand out, but I considered them still continuous.&lt;/p&gt;

&lt;h3 id=&quot;number-of-movies-by-genres&quot;&gt;Number of movies by genres&lt;/h3&gt;
&lt;p&gt;For each genre, I calculated the number of movies involving it.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Generate a long format dataframe 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;genres&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Count the number of inclusion of each genre.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;genres&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_counts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;notebook_output&quot;&gt;
&lt;pre&gt;
Drama              1746
Comedy             1347
Thriller           1197
Action             1078
Adventure           746
Romance             660
Crime               649
Science Fiction     518
Horror              460
Family              423
Fantasy             393
Mystery             344
Animation           200
Music               134
History             129
War                 119
Western              52
Documentary          34
Foreign              13
TV Movie              1
Name: genres, dtype: int64
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I noted that the bottom four genres are involved by a quite small number of movies, less than 100. The number of Western-involved movies is fewer than the half of War-involved movies. I decided to exclude these genres.&lt;/p&gt;

&lt;h3 id=&quot;vote-scores&quot;&gt;Vote scores&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/ol_vote.png&quot; alt=&quot;Boxplot of vote score&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Judging that it is so far away from the nearest, I removed the data point with the lowest value.&lt;/p&gt;

&lt;h3 id=&quot;rois&quot;&gt;ROIs&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/ol_ROI.png&quot; alt=&quot;Boxplot of ROI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The top two data points were considered too far away that I removed them.&lt;/p&gt;

&lt;p&gt;To sum up, the data wrangling step removed 7138 records leaving 3728 records in total. The largest removal, 7011 records, was due to value missingness.&lt;/p&gt;

&lt;h1 id=&quot;univariate-exploration&quot;&gt;Univariate Exploration&lt;a class=&quot;anchor&quot; id=&quot;ch3&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;I surveyed the variables, budget, ROI, genres, and vote score, individually, before looking at relationships between them.&lt;/p&gt;

&lt;h2 id=&quot;budget-1&quot;&gt;Budget&lt;/h2&gt;
&lt;p&gt;The budget is long right tailed, ranging from $8081 to $368 millon (redline for median).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/uni_budget.png&quot; alt=&quot;Budget histogram (Cleaned Dataset)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For later analysis, I categorized values into four groups as follows.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;qt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;budget_adj&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;binning4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Low&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Moderate&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;Little High&apos;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;qt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;High&apos;&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;budget_level&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;budget_adj&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binning4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/uni_budget_level.png&quot; alt=&quot;Median and Mean of Budget by Budget Level (Cleaned Dataset)&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;roi&quot;&gt;ROI&lt;/h2&gt;
&lt;p&gt;The ROI column is extremely long tailed, ranging from -1 to 699. More than 3500 out of 3728 fall into between -1 and 22.3 as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/uni_roi_histogram.png&quot; alt=&quot;ROI histogram (Cleaned Dataset)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I categorized the ROI in the same way to the budget.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/uni_roi_levels.png&quot; alt=&quot;Median and Mean of ROI by ROI Level (Cleaned Dataset)&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;genres&quot;&gt;Genres&lt;/h2&gt;
&lt;p&gt;I plotted the percentage of movies involving each genre. It shows that almost the half of movies involved ‘Drama’, and ‘Comedy’ is the next most involved genre by about %35.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/uni_genres.png&quot; alt=&quot;Percentage of movies by genre (Cleaned Dataset)&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;vote-score&quot;&gt;Vote score&lt;/h2&gt;
&lt;p&gt;The vote score seems normally distributed (mean = 6.2, SD = 0.8).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/uni_vote.png&quot; alt=&quot;Histogram of vote scores (Cleaned Dataset)&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;q1-did-high-budget-movies-achieve-high-roi&quot;&gt;Q1: Did high budget movies achieve high ROI?&lt;a class=&quot;anchor&quot; id=&quot;ch4&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;I plotted a scatter plot (top), ROI distribution by budget level (bottom-left), and median value by budget level (bottom right).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/bi_budget_ROI_plots.png&quot; alt=&quot;Budget and ROI plots (Cleaned Dataset)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is hard to find a relationship in the scatter plot. But, the boxplot by the budget level shows that ROI values of low budget movies span a wide range, up to 500, the largest ROI value in the dataset. The larger budget groups have the narrower spread of ROIs. As well, the comparison of median values shows that the low and high budget levels have a little higher median values than the moderate and little high budget levels.&lt;/p&gt;

&lt;p&gt;Meanwhile, I created a contigency table and plotted it as below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/bi_budget_ROI_con.png&quot; alt=&quot;Number of movies by ROI level for each budget level&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is found that low budget movies achieve the low and high level ROIs more frequently than the moderate and little high level ROIs. As opposed to it, high budget movies obtain the moderate and little high level ROIs more frequenlty.&lt;/p&gt;

&lt;h1 id=&quot;q2-did-specific-genres-lead-to-high-roi&quot;&gt;Q2: Did specific genres lead to high ROI?&lt;a class=&quot;anchor&quot; id=&quot;ch5&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;To compare ROIs between the genres, I calculated two types of means:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Simple mean: Simply add up all ROIs and divide it with the total number of movies involving that genre.&lt;/li&gt;
  &lt;li&gt;Weighted mean: Assign weights based on the number genre types involved in a movie. A ROI is multiplied by 1/number_of_genres and the sum is divided by the sum of 1/number_of_genres values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, let’s assume we have a long format dataframe of three movies as below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;genre&lt;/th&gt;
      &lt;th&gt;ROI&lt;/th&gt;
      &lt;th&gt;number of genres&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;drama&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;comedy&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;comedy&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;drama&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Simple means are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;drama, (3+2) / 2 = 2.5&lt;/li&gt;
  &lt;li&gt;comedy, (3+5) / 2 = 4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Weighted means are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;drama, (3x1/2+2x1/1) / (1/2+1) = (1.5+2) / 1.5 = 2.33&lt;/li&gt;
  &lt;li&gt;comedy, (3x1/2+5x1/1) / (1/2+1) = (1.5+5) / 1.5 = 4.33&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The below shows results of the two methods. The results are not exactly the same (especailly, ‘Horror’ has a larger difference between the two results than other genrens), but  order is the same.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/bi_genre.png&quot; alt=&quot;Mean of ROIs by genre (Cleaned Dataset)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The results hints that ‘Horror’ tends to bring quite larger ROIs compared to the others. The next profitable genre is ‘Music’ by round 4 or 5 smaller ROI than Horror. ROI values gradually decrease after ‘Music, and thus the difference between ‘Music’ and ‘History’ is only about 3.5.&lt;/p&gt;

&lt;p&gt;Although I believe that I obtained an acceptable finding with my approach, it is a shame that effect of combination of genres was not studied. This issue will be discussed a bit more in ‘Limitations’ section at the end.&lt;/p&gt;

&lt;h1 id=&quot;q3-did-high-score-movies-achieve-high-roi&quot;&gt;Q3: Did high score movies achieve high ROI?&lt;a class=&quot;anchor&quot; id=&quot;ch6&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/blog-site/assets/img/2022-06-01-what-are-associated-with-profitable-movies/bi_vote.png&quot; alt=&quot;Vote score distribution by ROI level (Cleaned Dataset)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The chart shows that the high ROI level tends to bring a little higher median of vote scores (0.4 higher than the little high level, and 0.7 than the low level).&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;a class=&quot;anchor&quot; id=&quot;ch7&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;With the exploration, I found association between 1) budget size and ROI, 2) genres and ROI, and 3) vote average score and ROI. To sum up:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I found contrary behavior between the low and high budget groups, in terms of their ROIs. Low budget movies are more likely to be big fail or big success, and high budget movies tend to end up in the middle. This behavior seems a consequence. Considering different capabilities in reaching the audience and the definition of ROI, this behavior seems natural.&lt;/li&gt;
  &lt;li&gt;The genre type that resulted in the highest ROI on average is ‘Horror’, and the smallest ROI, ‘History’,  among 17 genre types (excluding ‘Western’, ‘Documentary’, ‘Foreign’, and ‘TV Movie’).&lt;/li&gt;
  &lt;li&gt;It was found that the higher ROI level group tend to associate with higher mean of the vote average.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on these findings, it is considered that production of low budget, horror movies that receive high score from the audience is likely to lead to high ROI.&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;The exploration has several limitations as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Only about a third of the original datapoints were used in the analysis, since the rest had missing or outlying values. As well, there was no further work to check if any pattern exists in missing or outlying values.&lt;/li&gt;
  &lt;li&gt;In finding association between genres and ROIs, potential effect of genre combination was not explored, and it makes the result somewhat insecure. For instance, let’s assume the following dataset of four movies:&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;genres&lt;/th&gt;
      &lt;th&gt;ROI&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Drama&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Drama, Family&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Drama, Horror&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Family&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Calculation of weighted means (Drama, 5; Family, 4; Horor, 8) indicates that the highest ROI genre is Horror. However, it is not sure if this is the power of the Horror genre or result of pairing with Drama, since it does not have movies that involve only Horror.&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Jun 2022 00:00:00 -0700</pubDate>
      </item>
    
  </channel>
</rss>
