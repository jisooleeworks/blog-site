<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Search | Jisoo Lee</title>
	<meta name="description"
		content="Data analytics blog">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog-site/assets/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog-site/assets/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="/blog-site/search.html">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Jisoo Lee"
		href="/blog-site/feed.xml" />

	<!-- Font Awesome -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css"
		integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet"
		type="text/css">
	

	<!-- KaTeX -->
	

	<!-- Google Analytics -->
	

<!-- Begin section for dataframe table formatting -->
<style type="text/css">
    table.dataframe {
        width: 100%;
        height: 240px;
        display: block;
		overflow: auto;
        font-family: Arial, sans-serif;
        font-size: 0.7em;
        line-height: 1.2;
        text-align: center;
    }
    table.dataframe th {
      font-weight: bold;
      padding: 0.3em;
    }
    table.dataframe td {
      padding: 0.3em;
    }
    table.dataframe tr:hover {
      background: #b8d1f3; 
    }


	table.dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    table.dataframe tbody tr th {
        vertical-align: top;
    }

    table.dataframe thead th {
        text-align: left;
    }

	.notebook_output pre {
		font-size: 0.8em;
		border: 1;
		line-height: 1.2;
	}

	.highlighter-rouge pre{
		margin-bottom: 1.5em;
		padding: 1.2em;
	}

</style>
<!-- End section for dataframe table formatting -->

</head>
  <body>
    <header class="site-header">
	<div class="branding">
		
		<h1 class="site-title">
			<a href="/blog-site/">Jisoo Lee</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
			
			
			
			
			
			<li>
				<a class="page-link" href="/blog-site/about/">
					About
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled  -->
			














<li>
	<a href="https://github.com/jisooleeworks" title="Follow on GitHub">
		<i class="fab fa-fw fa-github"></i>
	</a>
</li>































            <!-- Search bar -->
            
		</ul>
	</nav>

</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog-site/')">
    <h1 class="title">Search</h1>
    
  </header>
  <section class="post-content"><div class="search">
    <div id="search-results"></div>
    <p id="not-found" style="display: none">
        No results found.
    </p>
</div>


<script>
  window.store = {
    
      "2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components-html": {
        "title": "Implementing Hierarchical and KMeans Clustering on Principal Components",
        "tags": "",
        "date": "September 18, 2022",
        "author": "",
        "category": "",
        "content": "Let’s say, you want to identify groups of customers to treat each group with different strategies. If you have a dataset containing features describing customers’ various aspects such as age, occupation, living area, spending behavior and so on, clustering methods can help you figure it out. Hierarchical clustering and KMeans clustering combined with PCA (Principal Component Analysis) are often used in various clustering methods. In this post, I will present how to implement hierarchical clustering, KMeans clustering, and both on PCA features, using SciPy and Scikit-learn libraries in Python.This description is guided by Asish Biswas’ worked example [1, 2, 3], which aims to show you how to apply the STP (Segmentation, Targeting, Positioning) marketing framework. The dataset used contains demographic information of 2000 customers, as follows:  Sex – categorical, 0: male, 1: female  Marital status – categorical, 0: single, 1: non-single  Age – numerical  Education – categorical, 0: unknown, 1: high school, 2: university, 3: graduate  Income – numerical  Occupation – categorical, 0: unemployed, 1: official, 2: management  Settlement size – categorical, 0: small city, 1: mid city, 2: big cityI will first talk about initial data exploration and do preprocessing. Then, I will show implementation of the clusterings one by one and then compare results of them. You can find full implementation and code in my github repository.Data Exploration and PreprocessingLet’s first look at several rows of the dataset to get familar with it.                  Sex      Marital status      Age      Education      Income      Occupation      Settlement size              ID                                                            100000001      0      0      67      2      124670      1      2              100000002      1      1      22      1      150773      1      2              100000003      0      0      49      1      89210      0      0              100000004      0      0      45      1      171565      1      1              100000005      0      0      53      1      149031      1      1      I’ve drawn plots to check distribution of each variable and relationship between pairs of the variables.The plots reveal existence of several correlations including relationship between the age and the education and between the occupation and the income.Since distance-based clustering methods are affected by difference of variable scales, I applied  standardization to the dataset. I picked the standardization as I will do PCA.from sklearn import preprocessingscaler = preprocessing.StandardScaler()data_sd = scaler.fit_transform(df)Hierarchical ClusteringI apply euclidean distance for a distance measure and Ward´s linkage for a linkage function.from scipy.cluster import hierarchyfrom scipy.spatial import distancelinkages_sd = hierarchy.linkage(data_sd, method='ward')plt.figure(figsize = (10, 7))hierarchy.dendrogram(linkages_sd,                     show_leaf_counts=False,                     no_labels=True)plt.title(\"Hierarchical Clustering Dendrogram\")The  dendrogram hints that making four groups may be reasonable.KMeans ClusteringTo determine the optimal number of clusters, I draw an elbow plot that tells the degree of variance explained according to the different number of clusters.from sklearn import clusterdistortions = []num_clusters = range(1, 11)for i in num_clusters:  centroids, labels, inertia = cluster.k_means(data_sd, n_clusters = i, random_state=42)  distortions.append(inertia)  labels_list.append(labels)  elbow_plot = pd.DataFrame({'num_clusters': num_clusters,                             'distortions': distortions})  sns.lineplot(x='Number of clusters', y='Sum of squared distances', data = elbow_plot, marker='o')  plt.xticks(num_clusters)  plt.title(\"KMeans Clustering Elbow Graph\");If comparing the magnitude of the slope between two numbers, it tends to remain the same from four. So, it is considered best to make four groups.PCAIn identifying principal components, let’s first import the sklearn library and create a PCA object with the scaled dataset.from sklearn import decomposition pca = decomposition.PCA()pca.fit(data_sd)We draw a plot that shows the degree of the explained variance accumulated with additional component.plt.figure(figsize=(9, 5))plt.plot(range(1, 8), pca.explained_variance_ratio_.cumsum(), marker='o')plt.xlabel('Number of components')plt.ylabel('Cumulative explained aariance')plt.title(\"Cumulative Explained Variance by Number of Principal Components\");In the above plot, we can see that the first three components explain 80% of the variance, which is considered acceptable [4]. So, I make an instance of the model with three for a number of components and fit it on the dataset. Then, I make a dataframe and heatmap with it to see a loading score of each variable for each component.pca = decomposition.PCA(n_components=3)pca.fit(data_sd)df_pca_components = pd.DataFrame(    data=pca.components_.round(4),    columns=df.columns.values,    index=['component 1', 'component 2', 'component 3'])s = sns.heatmap(    df_pca_components,    vmin=-1,    vmax=1,    cmap='RdBu_r',    annot=True)plt.title('Correlation Matrix for Principal Components and Original Features');With the above heatmap, we can see that:  Component 1 has a positive association with Income, Occupation, and Settlement Size  Component 2 has a positive association with Sex, Martital Status, and Education  Component 3 has a positive association with Age and a negative association with Martital Status and Occupation.Hierarchical Clustering and KMeans Clustering on PCA FeaturesLet’t run a hierarchical clustering model and KMeans on the three principal components.I first obtain a new dataset by transforming the original scaled dataset on the three principal components and then run a model on it.pca_scores = pca.transform(data_sd)linkages_pca = hierarchy.linkage(pca_scores, method='ward')I do not bring a dendrogram of the result here; you can find it in my github repository. If you see it, you will find that making four groups is a reasonable decision.I plot the segments with respect to the first two components as below.labels_hc_pca = hierarchy.fcluster(linkages_pca, t=4, criterion='maxclust')df_hc_pca = pd.concat([df.reset_index(drop=True), pd.DataFrame(pca_scores)], axis=1)df_hc_pca.columns.values[-3:] = ['component 1', 'component 2', 'component 3']df_hc_pca['label'] = labels_hc_pcaplt.figure(figsize=(10, 8))sns.scatterplot(    x=df_hc_pca['component 1'],    y=df_hc_pca['component 2'],    hue=df_hc_pca['label'],    palette=['b','m','g','r'])plt.title('Hierarchical Clustering on Principle Components');I think the segments are well seperated in overall, although the label 4 overlaps the label 2 and 3 a bit.KMeans Clustering on PCA FeaturesFinally, we run a KMeans clustering model on the three principal components. Based on the elbow graph that you can find in the github repository, I pick four as a number of clusters.centroids, labels, inertia = cluster.k_means(pca_scores, n_clusters = 4, random_state=42)ComparisonNow, we have four different versions of segmentation on the dataset. Below is a summary of each version (mean value of each variable for each cluster), in order of hierarchical clustering, KMeans clusteirng, hierarchical clustering with PCA, and KMeans clustering with PCA.Before getting the above, I’ve printed out a raw result of each version, done some eyeballing, and re-ordered it based on the age and income variable so that:  Label 1, the youngest and the third level of income  Label 2, in the middle level in age and the lowest level of income  Label 3, in the mdidle level in the age and the second level of income  Label 4, the eldest and the the top level of incomeThis segmentatation accord with all the four versions; e.g., the youngest group’s income rank is third in all the versions. The mean age values are simiar each other. But the mean income values are fairly much different. As well, we can see discordance between the versions in the other variables.I am going to do further analysis on the segmentation done with the Kmeans clustering on principal components.Exploration of SegmentsIn this section, I will look into the segments created with the Kmeans clustering on principal components to identify characteristics of each of them.If we compare the median values of the Age, Segment 4 is distinctively high, Segment 2 and 3 are similar, and Segment 1 is the lowest. However, we can see their overlapping with the long whiskers and outliers. The Income plot confirms that Segment 4 is higher than the others, especially Segment 3. However, we can see the wide range of Segment 4 with some extreme outliers on the top and the minimum value lower than Segment 3’s.The most distinctive aspect of Segment 1 is that they are mostly high school graduated non-single women in their late 20s. Two-thirds of them live in the small city, half of which have no job. A third live in the mid-sized city with ‘official’ jobs.Most people of Segment 2 are single living in the small city in the mid 30s. Two-thirds are male and a third, female. Two-thirds are unemployed.People of Segment 3 are male living the mid or big-sized city with job. The majority is single.All people of Segment 4 were graduated from university or the higher. Two-thirds are non-single, and a third, single. The sex ratio is one to one. They distributed to all the sizes of the city although more people live in the mid or big-sized city.Wrapping UpI have showed how to implement the main clustering methods, hierarchical and KMean clustering combined with PCA. When use the hierarchical clustering, you find out the appropriate number of clusters by looking at a dendrogram. If use the KMean clustering, the elbow graph can help you pick the number of clusters. We have found that the obtained segments are distintive each other, and it suggests usefulness of the clustering methods in segmenting customers.References  Asish Biswas. Customer Segmentation with Python (Implementing STP framework - part 1/5)  Asish Biswas. Customer Segmentation with Python (Implementing STP Framework - Part 2/5  Asish Biswas. Customer Segmentation with Python (Implementing STP Framework - Part 3/5)  Elitsa Kaloyanova. What Is Principal Components Analysis?",
        "url": "/blog-site//2022/09/18/implementing-hierarchical-and-kmeans-clustering-on-principal-components.html"
      }
      ,
    
      "2022-09-09-feature-scaling-min-max-z-score-html": {
        "title": "Feature Scaling, Min-Max and Z-Score Normalization",
        "tags": "",
        "date": "September 9, 2022",
        "author": "",
        "category": "",
        "content": "Feature scaling is making features take similar ranges of values. In this post, I will talk about when it is necessary and then look into two most frequently mentioned methods, min-max normalization and z-score nomalization. Finally, several other tehniques will be introduced briefly including mean normalization, robust scalar, and scaling to unit length. Examples accompany to help understand effects of each method, which are made with the Python and scikit-learn APIs.Before diving into details, I would like to talk about the confusion due to naming discrepancies. Given the below, it seems that ‘scaling’ is ‘normalizing’, and ‘normalizing’ is ‘scaling’:  “Feature scaling is a method used to normalize the range of independent variables or features of data.” [1]  “The goal of normalization is to transform features to be on a similar scale.” [2]People often use them interchangeably (e.g, Min-Max Normalization [3] and Min-Max Scaling [4])Meanwhile, we can find many documents online that use the term, ‘normalization’, to indicate a particular scaling/normalizing method that is often called ‘min-max scaling’ or ‘min-max normalization’, typically contrasting it to other scaling/normalizing method, ‘standardization’, which is also called ‘z-score normalization’ [5][6][7].Given the inconsistent naming, we may need some extra info to figure out what exactly it means when we run into the term, ‘normalization’, in materials dealing with feature scaling.When should we do feature scaling?There are two cases often mentioned as reasons for scaling: (1) to prevent feature bias when using distance-based models, and (2) to improve the performance of gradient descent [1][8][9].  Distance-based modelsModels that use distances between data points like KNN, K-means, PCA, and SVM should do normalization. If there is a feature having a wide range of values, the distance will be dominated by this feature. So we make features be on a similar scale that each feature contributes approximately proportionately to the final distance. You can witness its benefit in the example made by Brownlee; it demonstarates that the KMM modeling brings different accuracy results with a not-scaled (that is, raw) and scaled data.  Gradient descent-based optimization algorithmsScaling brings benefit to models like linear regression that use gradient descent for optimization, since gradient descent converges much faster.MethodHow can we make features take similar ranges of values? While there are many others, you may run into Z-score normalization and Min-Max normalization most often when you search for feature scaling methods. I cautiously anticipate that they are most often used in practices. Let’s first look at the brief description of each of them and then comparisons focusing on when to use which one.Z-score normalization (often called Standardization)This rescales the ranges of features so that they have distribution with 0 mean value and 1 standard deviation value. The formula is:\\[x' = \\frac{x-\\bar{x}}{\\sigma}\\]Min-Max normalizationThis rescales values to a fixed range, usually [0, 1] or [-1, 1]. The formula for a min-max of [0, 1] is:\\[x' = \\frac{x-min(x)}{max(x) - min(x))}\\]The formula to rescale to a range of [a, b] is:\\[x' = a + \\frac{(x-min(x))(b-a)}{max(x) - min(x))}\\]Z-score normalization vs. Min-Max normalization(1) Min-Max normalization is affected by outliers much, and Standardization is much less affected by outliers. I found a nice demonstration on this aspect in the Codecademy website [10]. Here is my replication and summary of it. First, suppose a dataset of 100 houses with two variables, the number of rooms and the total years of the house. The number of rooms ranges between 1 and 20. The years of 99 houses range between 0 and 40, and one is 100 years old – an outlier. The below compares results of scaling:With min-max normalization, the 99 values of the age variable are located between 0 and 0.4, while all the values of the number of rooms are spread between 0 and 1. With z-score normalization, most (99 or 100) values are located between about -1.5 to 1.5 or -2 to 2, which are similiar ranges.Note: I ran into a blog post saying that Min-Max normalization is not affected by outliers and Standardization is. I guess that the author means the bounding range of Min-Max normalization results and the unbounding range of Standardization.(2) Often, Standardization is recommended to use when the feature distribution is Normal or Gaussian, and Min-Max normalization, when it is not [11]. Min-Max normalization just changes the range of data and Standardization changes it radically, that is, does not transform the underlying distribution structure of the data. I interprete that this is why it is OK to apply Standardization to normally distributed data (since the normal data would end up the normal data), but not recommended to apply it to non-normal data.However, it seems there is no hard and fast rules. Here is Brownlee’s suggestion [11]:  Predictive modeling problems can be complex, and it may not be clear how to best scale input data. If in doubt, normalize the input sequence. If you have the resources, explore modeling with the raw data, standardized data, and normalized data and see if there is a beneficial difference in the performance of the resulting model.Let’s look at a few other scaling methods.Mean normalizationThis rescales the values to a range of [-1, 1] with 0 average value. That is, it results in a distribution centered at 0, with its minimum and maximum values within the range of -1 to 1.\\[x' = \\frac{x-\\bar{x}}{max(x) - min(x)}\\]Robust Scalar (Scaling to median and quantiles)This uses the median and quantiles. It is less affected by outliers than the Min-Max.\\[x' = \\frac{x-x_{median}}{IQR}\\]Scaling to unit lengthIt rescales the components of a feature vector so that the length of the scaled vector is 1. One of two norm types is used; that is, divide each compoment by either the Manhattan distance (l1 norm) or the Euclidean distance (l2 norm) of the vector.Wrapping upI hope that I perform deeper survey and enough experiments to figure out kind of my own guidance, if any, for choosing a suitable scaling method like Hale is doing[12], although it seems not achievable to definite rules[13]. However, knowing on the underlying math behind models\\scaling methods will be definitly helpful in saving time and understanding results.References  https://en.wikipedia.org/wiki/Feature_scaling  https://developers.google.com/machine-learning/data-prep/transform/normalization  https://www.geeksforgeeks.org/ml-feature-scaling-part-2/  https://medium.com/@isalindgren313/transformations-scaling-and-normalization-420b2be12300  https://www.statology.org/standardization-vs-normalization/  https://www.geeksforgeeks.org/normalization-vs-standardization/  https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf  https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/  https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e  https://www.codecademy.com/article/normalization  https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/  https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02  https://www.kdnuggets.com/2019/04/normalization-vs-standardization-quantitative-analysis.html/2",
        "url": "/blog-site//2022/09/09/feature-scaling-min-max-z-score.html"
      }
      ,
    
      "2022-07-25-sample-size-formulas-html": {
        "title": "Sample size formulas",
        "tags": "",
        "date": "July 25, 2022",
        "author": "",
        "category": "",
        "content": "Although online tools are available to obtain a sample size,  I make a note of formulas for sample size calculation. Being aware of them may help you use online calculators more confidently. It seems that formulas come in different flavors. However, there are more commonly used ones. I will list them up, after check the fundamentals of sample size determination.Factors to consider in sample size determinationBefore formulas, let’s look through what affect the sample size:  It depends on the size of difference that you would like to detect. If differences are smaller, a larger sample size is required. That is, you may not be able to detect a small difference (e.g., cannot reject the null hypothesis), if a sample size is not large enough for it. In other words, larger differences can be caught with smaller sample sizes.  It depends on the standard deviation. The smaller the standard deviation is, the smaller the sample size it.  It depends on the size of Type 1 error. If you are strict on it and thus set it smaller, you need a larger sample size.  It depends on the size of Type 2 error. If you set it smaller, a larger sample size is required. In other words,  you are likely to fail to reject the null hypothesis, if your sample size is not large enough for your Type 2 error level.In practices, the third and forth factor seems easy to determine. I may follow kind of conventions: e.g., 0.05 or 0.01 for Type 1 error, and 0.1 or 0.2 for Type 2 error. But, the difference and standard deviation are tricky. With regard to the difference, we need to think of what meaningful difference is for a given situation. For instance, “is it worthwhile conducting a study to see if there is 0.1% increase in conversion rate?” With regard to estimation of the standard deviation, it is suggested to refer to existing work, or conduct pilot test, if feasible, and with caution [1][2].For the meanFormulas for sample size calculation have different detail according to study designs. We will first look at formulas for the mean to give you a sense of how they are formed fundamentally.Finding a confidence intervalSuppose you target a margin of error that is calculated as follow:\\[E = Z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\]By solving this for n using algebra, you get the below formula that results in the sample size needed for the target margin of error:\\[n = \\frac{Z_{1-\\alpha/2}^2\\sigma^2}{E^2}\\]Hypothesis testingWe consider not only Type 1 error, but also Type 2 error. In general, the formula for power computation involving the both is as follow (for the two-tailed test) [3]:\\[Z_{power} = \\frac{\\delta}{SE} - Z_{1-\\alpha/2}\\]One sample testFor the one sample t-test where \\(\\delta = \\lvert\\bar{x} - \\mu\\rvert\\), we plug the below and solve for n with some algebra:\\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]The derived formula is:\\[n =\\frac{(Z_{1-\\alpha/2} + Z_{power})^2\\sigma^2}{\\delta^2}\\]Two sample testFor the two sample test where \\(\\delta = \\lvert\\bar{x_1} - \\bar{x_2}\\rvert\\), we plug the below:\\[SE = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{rn_2}},\\newline\\text{where r is ratio of group 2 to group 1 size}\\]With some algebra, we can obtain the following [4]:\\[n_1 = \\frac{(r+1)}{r}\\frac{\\sigma^2(Z_{power}+Z_{\\alpha/2})^2}{\\delta^2}\\]If r=1 (equal groups), then\\[n_1 = \\frac{2\\sigma^2(Z_{power}+Z_{\\alpha/2})^2}{\\delta^2}\\]Paired testThe formula for the paired test is [4]:\\[n = \\frac{\\sigma_d^2(Z_{power}+Z_{\\alpha/2})^2}{\\delta^2},\\newline\\text{where}~\\sigma_d^2~\\text{is the standard deviation of the within-pair difference.}\\]For the proportionFor a study to get a confidence interval,\\[n = \\frac{Z_{1-\\alpha/2}^2p(1-p)}{E^2}\\]For one sample test,\\[n =\\frac{(Z_{1-\\alpha/2} + Z_{power})^2p(1-p)}{\\delta^2}\\]For two sample test [4],\\[n_1 = \\frac{(r+1)}{r}\\frac{(Z_{power}+Z_{\\alpha/2})^2\\{p_1(1-p_1)+p_2(1-p_2)\\}}{\\delta^2}\\]Note:  All the above with regard to the hypothesis testing assumes two-tailed test; if we do one-tailed test, \\(Z_{1-\\alpha}\\) used, intead of \\(Z_{1-\\alpha/2}\\).  If the standard deviation of the population is unknown and the sample size is small, the formulation depends on the t distribution. It causes a problem because critical values of the t distribution depneds on degrees of freedom, which depends on the sample size which we are trying to estimate. This document describes how to manage this issue.AdjustmentsWhile we have looked around a collection of formulas and may feel quite ready, we need to be aware of situations where those formulas will not work well, and some adjustment is necessary [5].When we draw a sample from a finite small population, it is suggested to apply the finite population correction factor. The derived formula is as follows [6]:\\[n_{finit\\_pop} = \\frac{n}{1+(\\frac{n-1}{N})} = \\frac{nN}{n+(N-1)}\\newline\\text{where N is the population size.}\\]Wrapping upWe have looked around formulas for sample size calculation that are most commonly mentioned. Athough a bit of difference exists in detail according to study designs, they share the same fundamental that was reviewed at the beginning of this post. Although the knowledge covered here may be a small fraction of the whole body of work on the sample size, it should be fairly enough for confident use of existing tools.Reference:  Whitehead AL, Julious SA, Cooper CL, Campbell MJ. Estimating the sample size for a pilot randomised trial to minimise the overall trial sample size for the external pilot and main trial for a continuous outcome variable. Stat Methods Med Res. 2016 Jun; 25(3):1057-73. (link)  Bell ML, Whitehead AL, Julious SA. Guidance for using pilot studies to inform the design of intervention trials with continuous outcomes. Clin Epidemiol. 2018;10:153-157. (link)  WISE, Claremont Graduate University. Power Calculations for One-Sample Z Test in Power Tutorial. (link)  Kristin L. Sainani. Lecture 11 of course hrp259. (link)  Monitoring &amp; Evaluation. Sample Size Calculation. (link)  Cochran, William G. Sampling techniques. John Wiley &amp; Sons, 1977.",
        "url": "/blog-site//2022/07/25/sample-size-formulas.html"
      }
      ,
    
      "2022-07-15-hacker-statistics-simulation-of-data-acquisition-html": {
        "title": "Hacker Statistics, simulation of data acquisition",
        "tags": "",
        "date": "July 15, 2022",
        "author": "",
        "category": "",
        "content": "I will share the concept and applications of “hacker statistics”, an approach for statistical inference. I first try to identify its definition, and then present several examples that apply it. It is considered a quite versitile tool, although there remain a number of questions to study further.What is hacker statistics? My online search did not bring much information on it, and it looks like  there is no established definition. But heavily influenced by Justin Bois’s tutorial[1], I understand it as “simulating data acquisition under an assumption to find the probability of an aspect of interest.”  So, next, I will present examples found in Justin Bois’s tutorial[1].ExamplesWhat is the probability of getting 4 heads when a coin with probability 0.5 of heads is tossed 4 times?To get an answer to it following the spirit of hacker stats, we do a lot of 4-flip trial recording the number of heads each trial to get an answer, rather than rely on any formula. But, instead of manually doing it, we can realize a lot of coin flipping using a random number generator and for loop. The below code generates 10000 repeats of the 4-flip trial, counting the number of heads. We get the number of  trials with  4 heads, and finally the probability of getting 4 heads in 4 tosses.# Code from [1]n_all_heads = 0 # Count number of 4-head trialsfor n in range(10000): # 10000 repeats of the four flip trials    heads = np.random.random(size = 4) &lt; 0.5    n_heads = np.sum(heads)    if n_heads == 4: # If a given trial has four heads, increase the count        n_all_heads += 1        n_all_heads / 10000 # What is the probability of getting all four heads?What is the confidence interval of a sample?Suppose we have an observed data set of the rainfall amount in millimeters ([122.1,  69.8,  29.6, …,  65.8,  58.2, 130.4]). To get a confidence interval of the mean using hacker stats, we need to be able to simulate repeated measurements, and here comes  ‘bootstrapping’, sampling a dataset with replacement.The below code implements bootstrapping using np.random.choice() in a for loop. It generates 10000 samples, and calculates the mean of each of them. That is, a sampling distribution of the mean is made, and a confidence interval is found by looking at 2.5 and 97.5th percentiles.bs_replicates = np.empty(10000)for i in range(10000):        # Generate a bootstrap sample    bs_sample = np.random.choice(rainfall, size = len(rainfall)) # Resampling engine    bs_replicates[i] = np.mean(bs_sample)    # 95 percent confidence intervalconf_int = np.percentile(bs_replicates, [2.5, 97.5])Is the amount of rainfall in November and June different?Suppose we have a dataset, a record of the amount of rainfall during November and June. To see if these months have different amount of rainfall indeed, we set the following hypothesis to test:\\[H_0: \\mu_{nov} = \\mu_{jun}\\newlineH_1: \\mu_{nov} \\not = \\mu_{jun}\\]To do the test with hacker stats, we need to generate many sets of data considering the null hypothesis is true. As a simulation method, there is permutation resampling: merge the two data sets, and iteratively divide the all values into two groups of the same sizes as the two data sets, in every possible way (every possible permutation)[2]. For each iteration, the difference of the two groups’ means is calculated, and we finally check where the observed difference locates in the distribution of these simulated differences.diff_observed = np.mean(rain_june) - np.mean(rain_november)perm_replicates = np.empty(10000)for i in range(10000): # 10000 iterations    # Generate a permutation sample    perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november)    # Compute the test statistic    perm_replicates[i] = np.mean(perm_sample_1) - np.mean(perm_sample_2)# Plot the histogramplt.hist(perm_replicates, density=True)plt.xlabel('Mean difference')plt.ylabel('PDF')# Plot vertical lines at the absolute of the observed difference and negative to itplt.axvline(np.abs(diff_observed), color = 'r')plt.axvline(np.abs(diff_observed)*-1, color = 'r')empirical_diff_means = np.mean(rain_november) - np.mean(rain_june)p = np.sum(perm_replicates &gt;= empirical_diff_means) / len(perm_replicates)For a p-value, we get the proportion of the distribution where the absolute difference was greater than the absolute value of the observed difference (as the example is the two-tailed test).We have looked around the examples that apply the hacker stats technique for different questions, and I believe that you got an idea on what hacker stats is.To the best of my knowledge, there are three methods that realize hacker stats [1][3][5]:  Use of an existing model  Permutation sampling  BootstrappingGiven them, I wondered about how different results of them would be, when applied to the sample problem. Adapting materials that were found in Bois’ tutorial[1] and Udacity[5], I performed two sample z-test of proportions using each of the three methods.Two sample z-test of proportions (e.g., A/B testing of click-through rate)Suppose we have to determine whether the new banner design leads to a better click through rate (CTR) than the old one, and we have the below dataset that tells her group and whether she clicked or not, for each unique user (6328 rows × 2 columns).                  group      clicked                  5640      control      True              376      control      False              4247      control      True              2746      control      False              651      control      False              ...      ...      ...              5157      experiment      False              4490      experiment      False              7745      experiment      True              942      experiment      False              5255      experiment      True      A hypothesis has been made like this:\\[H_0: p_{exp} - p_{ctl} \\leq 0\\newlineH_1: p_{exp} - p_{ctl} &gt; 0\\]Permutation samplingFirst, I present use of permutation sampling, borrowing code from [1]. The below generates 10000 CTR differences between 10000 resampled pairs of the control and experiment groups.# ctl_df, a dataframe made by extracting control group records from 'both_df'# exp_df, experiment group recordsctl_ctr = np.sum(ctl_df['clicked']) / len(ctl_df)exp_ctr = np.sum(exp_df['clicked']) / len(exp_df)obs_diff = exp_ctr - ctl_ctrctl = ctl_df['clicked'].values exp = exp_df['clicked'].valuesdiffs_pm = np.empty(10000)for i in range(10000):    # Concatenate the data sets: data    data = np.concatenate((ctl, exp))    # Permute the concatenated array: permuted_data    permuted_data = np.random.permutation(data)    # Split the permuted array into two: perm_sample_1, perm_sample_2    ctl_perm_sample = permuted_data[:len(ctl)]    exp_perm_sample = permuted_data[len(exp):]        diff_sample = np.sum(exp_perm_sample) / len(exp_perm_sample) - \\                np.sum(ctl_perm_sample) / len(ctl_perm_sample)        # Compute the test statistic    diffs_pm[i] = diff_samplep = np.sum(diffs_pm &gt; obs_diff) / len(diffs_pm)print('p-value =', p)This is the histogram of the CTR differences (test statistics acquired with the resampling), with a line indicating the observed difference (p-value is 0.0027):We can get a p-value by dividing the number of differences larger than the observed difference by the total number of values.p = np.sum(diffs_pm &gt; obs_diff) / len(diffs_pm)print('p-value =', p)BootstrappingSecond, I test the hypothesis by generating samples using the bootstrap method.# 1) Compute the observed differenceobs_diff = exp_ctr - ctl_ctr# 2) Simulate the sampling distribution for the difference in proportionsdiffs_bt = []for _ in range(10000):    # Pick from the whole data with replacement    b_samp = both_df.sample(both_df.shape[0], replace = True)        ctl_samp = b_samp.query('group == \"control\"')    exp_samp = b_samp.query('group == \"experiment\"')        ctl_samp_ctr = ctl_samp['clicked'].sum() / len(ctl_samp)    exp_samp_ctr = exp_samp['clicked'].sum() / len(exp_samp)     diffs_bt.append(exp_samp_ctr - ctl_samp_ctr)Next, using the above sampling, a distribution under the null hypothesis is simulated, by creating a random normal distribution centered at 0, with the same spread and size.diffs_arr = np.array(diffs_bt)null_vals = np.random.normal(0, diffs_arr.std(), diffs_arr.size)plt.hist(null_vals)plt.axvline(x = obs_diff);To get a p-value:(null_vals &gt; obs_diff).mean()This is the histogram of the simulated CTR rate differences (p value is 0.0039):Use of binomial distributionAs shown below, sample acquistion from a binonimal distribution can be simulated using np.random.choice().# Calculate a click-through-rate under the null hypothesisp_null = np.sum(both_df['clicked']) / len(both_df)diffs_bd_1 = []for _ in range(10000):    new_page_converted = np.random.choice([1,0], size = n_exp, replace = True, p = (p_null, 1-p_null))    old_page_converted = np.random.choice([1,0], size = n_ctl, replace = True, p = (p_null, 1-p_null))    diffs_bd_1.append(new_page_converted.mean() - old_page_converted.mean())This is the histogram of the results (p value is 0.0049):Or, np.random.binomial() can be used as below.new_simulation = np.random.binomial(n_exp, p_null, 10000)/n_expold_simulation = np.random.binomial(n_ctl, p_null, 10000)/n_ctldiffs_bd_2 = new_simulation - old_simulationThis is the histogram of the results (p value is 0.0045):In addition to the use of hacker stats, I calculated a p-value using the parametric method (statsmodels.stats.proportion.proportions_ztest()).from statsmodels.stats.proportion import proportions_ztest, proportion_confintsuccesses = [928, 932]nobs = [n_exp, n_ctl]z_stat, pval = proportions_ztest(successes, nobs=nobs, alternative = 'larger')(lower_con, lower_treat), (upper_con, upper_treat) = proportion_confint(successes, nobs=nobs, alpha=0.05)print(f'z statistic: {z_stat:.2f}')print(f'p-value: {pval:.5f}')z statistic: 2.62p-value: 0.00442You can find all the code in my Github page.Let’s compare all the p-values:  Permutation method: 0.0027  Bootstrapping: 0.0039  Use of binomial distribution with np.random.choice(): 0.0045  Use of binomial distribution with np.random.binomial(): 0.0049And, with the parametric method, 0.00442Since we generated samples using random number generators, the results with the hackter stats methods are not fixed ones. I ran the code a few times, and compared them as below:perm 0.0027 &lt; boot 0.0039 &lt; para 0.00442 &lt; bi 0.0045 &lt; choice 0.0049choice 0.0026 &lt; perm 0.003 &lt; boot 0.0039 &lt; para 0.00442 &lt; bi 0.0054perm 0.0013 &lt; choice 0.0033 &lt; boot 0.0037 &lt; bi 0.0039 &lt; para 0.00442perm 0.0025  &lt; para 0.00442 &lt; choice 0.0046 &lt; boot 0.0051 &lt; bi 0.0053Given the results, the below questions came to my mind:  The p values are somewhat different each other. Which should we pick as a final result? Which one is the most trustable?  In my observation, the permutation sampling results in a smaller p-value than the bootstrapping. Will it be always true no matter what data are tested?References  Justin Bois. Datacamp. Statistical thinking (1) and Statistical thinking (2)  Wikipedia. Permutation test  Yevgeniy (Gene) Mishchenko. Bootstrapping vs. Permutation Testing  Matthew E. Parker. Resampling (Bootstrapping &amp; Permutation Testing)  Udacity, Data Analyst Nanodegree  Jake Vanderplas. Statistics for Hackers - PyCon 2016",
        "url": "/blog-site//2022/07/15/hacker-statistics-simulation-of-data-acquisition.html"
      }
      ,
    
      "2022-06-01-what-are-associated-with-profitable-movies-html": {
        "title": "What are associated with profitable movies?",
        "tags": "",
        "date": "June 1, 2022",
        "author": "",
        "category": "",
        "content": "Using a TMDb movie data set containing properties of movies released between 1960 and 2015, I explored what are related to financial success of movies, with visualizations.In assessing financial success, I used Return on Investment (ROI), a measure commonly used in describing profitability. It is calculated by: ROI = (gross revenue - cost)/cost. Of the properties, I was most interested in budget, genres, and vote scores. Specifically, I investigated:  Did high budget movies achieve high ROI?  Did specific genres lead to high ROI?  Did high score movies achieve high ROI?The work process was:  Data overview  Data wrangling  Univariate exploration  Q1: Did high budget movies achieve high ROI?  Q2: Did specific genres lead to high ROI?  Q3: Did high score movies achieve high ROI?  Conclusions (Summary, Limitations)I will walk through the process here. You can see code and all details in my GitHub.Data overviewAfter loading the data set, I looked at a couple of fundamental aspects.df.head()                  id      imdb_id      popularity      budget      revenue      original_title      cast      homepage      director      tagline      keywords      overview      runtime      genres      production_companies      release_date      vote_count      vote_average      release_year      budget_adj      revenue_adj                  0      135397      tt0369610      32.985763      150000000      1513528810      Jurassic World      Chris Pratt|Bryce Dallas Howard|Irrfan Khan|Vi...      http://www.jurassicworld.com/      Colin Trevorrow      The park is open.      monster|dna|tyrannosaurus rex|velociraptor|island      Twenty-two years after the events of Jurassic ...      124      Action|Adventure|Science Fiction|Thriller      Universal Studios|Amblin Entertainment|Legenda...      6/9/15      5562      6.5      2015      1.379999e+08      1.392446e+09              1      76341      tt1392190      28.419936      150000000      378436354      Mad Max: Fury Road      Tom Hardy|Charlize Theron|Hugh Keays-Byrne|Nic...      http://www.madmaxmovie.com/      George Miller      What a Lovely Day.      future|chase|post-apocalyptic|dystopia|australia      An apocalyptic story set in the furthest reach...      120      Action|Adventure|Science Fiction|Thriller      Village Roadshow Pictures|Kennedy Miller Produ...      5/13/15      6185      7.1      2015      1.379999e+08      3.481613e+08              2      262500      tt2908446      13.112507      110000000      295238201      Insurgent      Shailene Woodley|Theo James|Kate Winslet|Ansel...      http://www.thedivergentseries.movie/#insurgent      Robert Schwentke      One Choice Can Destroy You      based on novel|revolution|dystopia|sequel|dyst...      Beatrice Prior must confront her inner demons ...      119      Adventure|Science Fiction|Thriller      Summit Entertainment|Mandeville Films|Red Wago...      3/18/15      2480      6.3      2015      1.012000e+08      2.716190e+08              3      140607      tt2488496      11.173104      200000000      2068178225      Star Wars: The Force Awakens      Harrison Ford|Mark Hamill|Carrie Fisher|Adam D...      http://www.starwars.com/films/star-wars-episod...      J.J. Abrams      Every generation has a story.      android|spaceship|jedi|space opera|3d      Thirty years after defeating the Galactic Empi...      136      Action|Adventure|Science Fiction|Fantasy      Lucasfilm|Truenorth Productions|Bad Robot      12/15/15      5292      7.5      2015      1.839999e+08      1.902723e+09              4      168259      tt2820852      9.335014      190000000      1506249360      Furious 7      Vin Diesel|Paul Walker|Jason Statham|Michelle ...      http://www.furious7.com/      James Wan      Vengeance Hits Home      car race|speed|revenge|suspense|car      Deckard Shaw seeks revenge against Dominic Tor...      137      Action|Crime|Thriller      Universal Pictures|Original Film|Media Rights ...      4/1/15      2947      7.3      2015      1.747999e+08      1.385749e+09      df.info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 10866 entries, 0 to 10865Data columns (total 21 columns): #   Column                Non-Null Count  Dtype  ---  ------                --------------  -----   0   id                    10866 non-null  int64   1   imdb_id               10856 non-null  object  2   popularity            10866 non-null  float64 3   budget                10866 non-null  int64   4   revenue               10866 non-null  int64   5   original_title        10866 non-null  object  6   cast                  10790 non-null  object  7   homepage              2936 non-null   object  8   director              10822 non-null  object  9   tagline               8042 non-null   object  10  keywords              9373 non-null   object  11  overview              10862 non-null  object  12  runtime               10866 non-null  int64   13  genres                10843 non-null  object  14  production_companies  9836 non-null   object  15  release_date          10866 non-null  object  16  vote_count            10866 non-null  int64   17  vote_average          10866 non-null  float64 18  release_year          10866 non-null  int64   19  budget_adj            10866 non-null  float64 20  revenue_adj           10866 non-null  float64dtypes: float64(4), int64(6), object(11)memory usage: 1.7+ MBThe data set had 10866 records with 21 columns. Some columns, like ‘cast’ and ‘genres’, contained multiple values separated by pipe characters. With regard to budget and revenue, the data set had both original and inflation adjusted value column. I chose to use the inflation adjusted ones.Data wranglingIn this step, I dealt with:  Duplicates  Missing values  Unused columns  Incorrect values  Extraction of a variable of interest (i.e., ROI)  Data type  OutliersDuplicatesFirst of all, I examined if there were any duplicate records by checking for (1) duplicate id values, and (2) records with the same ‘title’, ‘director’, ‘release_year’, and ‘runtime’.Based on ‘id’df[df.duplicated(['id'], keep = False)]                  id      imdb_id      popularity      budget      revenue      original_title      cast      homepage      director      tagline      keywords      overview      runtime      genres      production_companies      release_date      vote_count      vote_average      release_year      budget_adj      revenue_adj                  2089      42194      tt0411951      0.59643      30000000      967000      TEKKEN      Jon Foo|Kelly Overton|Cary-Hiroyuki Tagawa|Ian...      NaN      Dwight H. Little      Survival is no game      martial arts|dystopia|based on video game|mart...      In the year of 2039, after World Wars destroy ...      92      Crime|Drama|Action|Thriller|Science Fiction      Namco|Light Song Films      3/20/10      110      5.0      2010      30000000.0      967000.0              2090      42194      tt0411951      0.59643      30000000      967000      TEKKEN      Jon Foo|Kelly Overton|Cary-Hiroyuki Tagawa|Ian...      NaN      Dwight H. Little      Survival is no game      martial arts|dystopia|based on video game|mart...      In the year of 2039, after World Wars destroy ...      92      Crime|Drama|Action|Thriller|Science Fiction      Namco|Light Song Films      3/20/10      110      5.0      2010      30000000.0      967000.0      Two records were detected, identical for all the variables. So, an extra was deleted.Based on ‘title’, ‘director’, ‘release_year’, and ‘runtime’df[df.duplicated(['original_title', 'director', ], keep = False)]\\    [['original_title', 'director', 'release_year', 'runtime']].sort_values(by=['original_title'])                  original_title      director      release_year      runtime                  1400      9      Shane Acker      2009      79              6514      9      Shane Acker      2005      11              4337      Bottle Rocket      Wes Anderson      1994      13              8547      Bottle Rocket      Wes Anderson      1996      91              4451      Frankenweenie      Tim Burton      2012      87              7943      Frankenweenie      Tim Burton      1984      29              4063      Madea's Family Reunion      Tyler Perry      2002      0              6701      Madea's Family Reunion      Tyler Perry      2006      110              5202      Saw      James Wan      2003      9              7011      Saw      James Wan      2004      103      The output shows that some movies have another movie with the same title/director/release_year. With online search, I learned that this happens sometimes; e.g., a director makes a ‘mini’ version before the full scale. So I kept all the records.Missing valuesThe ‘genre’ had only 23 nulls, but there were many records with 0 value for the budget and/or revenue. I had to remove all the records that belonged to any of the following:  null for ‘genre’  0 for ‘budget_adj’  0 for ‘revenue_adj’Incorrect valuesAs a minimum effort to ensure correctness of budget and revenue values, I inspected data points at both ends (the lowest and highest).Top budget moviedf[df['budget_adj'] == df['budget_adj'].max()]                  id      budget      revenue      original_title      genres      vote_average      release_year      budget_adj      revenue_adj                  2244      46528      425000000      11087569      The Warrior's Way      Adventure|Fantasy|Action|Western|Thriller      6.4      2010      425000000.0      11087569.0      Although the above says that ‘The Warrior’s Way’ is the highest budget movie by 425 million, online search revealed that the budget of it is 42.5 million. After this correction, the highest budget became $368 millon of ‘Pirates of the Caribbean: On Stranger Tides,’ which is correct.Top revenue movie‘Avartar’ came out as the highest revenue movie, and online search confirmed it.Bottom budget movieI displayed several lowest budget movies, and got puzzled with extremely low values like $1, $50, or ‘$100. I did online search and found that:  ‘Primer (2004)’ is considered as the lowest budget movie (https://en.wikipedia.org/wiki/Low-budget_film)  Many of the low budget values are wrong; for example, the budget of ‘Lost &amp; Found (1999)’ is $30 million, not $1.3, ‘Joyful Noise (2012)’, $25 millon, not $23, ‘Weekend (2011)’, $0.14 millon, not $7755, etc. With the above findings, I inclined to believe that $8081, budget of ‘Primer (2004)’, should be the lowest. I removed movies whose budget was lower than it.Bottom revenue movieAgain, online search made me distrust data points whose revenue was lower than the revenue of ‘Best Man Down’ ($1840.6). They were removed.Data typedf.loc[:, 'genres'] = df['genres'].str.strip().str.split(\"|\")Values of ‘genres’ transformed from string to list, e.g., Action|Adventure|Science Fiction –&gt; [Action, Adventure, Science Fiction], for convenience in later analysis.OutliersFundamentally, I identified records that might inhibit finding a general direction by looking into each variable, and removed them.BudgetSeveral records on the top stand out, but I considered them still continuous.Number of movies by genresFor each genre, I calculated the number of movies involving it.# Generate a long format dataframe df_long = df.explode('genres')# Count the number of inclusion of each genre.df_long['genres'].value_counts()Drama              1746Comedy             1347Thriller           1197Action             1078Adventure           746Romance             660Crime               649Science Fiction     518Horror              460Family              423Fantasy             393Mystery             344Animation           200Music               134History             129War                 119Western              52Documentary          34Foreign              13TV Movie              1Name: genres, dtype: int64I noted that the bottom four genres are involved by a quite small number of movies, less than 100. The number of Western-involved movies is fewer than the half of War-involved movies. I decided to exclude these genres.Vote scoresJudging that it is so far away from the nearest, I removed the data point with the lowest value.ROIsThe top two data points were considered too far away that I removed them.To sum up, the data wrangling step removed 7138 records leaving 3728 records in total. The largest removal, 7011 records, was due to value missingness.Univariate ExplorationI surveyed the variables, budget, ROI, genres, and vote score, individually, before looking at relationships between them.BudgetThe budget is long right tailed, ranging from $8081 to $368 millon (redline for median).For later analysis, I categorized values into four groups as follows.qt = df['budget_adj'].quantile([0.25, 0.5, 0.75])def binning4(x):    if x &lt; qt[0.25]:        return \"Low\"    elif (x &gt;= qt[0.25]) &amp; (x &lt; qt[0.5]):        return \"Moderate\"    elif (x &gt;= qt[0.5]) &amp; (x &lt; qt[0.75]):        return 'Little High'    elif x &gt;= qt[0.75]:        return 'High'    df['budget_level'] = df['budget_adj'].apply(binning4)ROIThe ROI column is extremely long tailed, ranging from -1 to 699. More than 3500 out of 3728 fall into between -1 and 22.3 as shown below.I categorized the ROI in the same way to the budget.GenresI plotted the percentage of movies involving each genre. It shows that almost the half of movies involved ‘Drama’, and ‘Comedy’ is the next most involved genre by about %35.Vote scoreThe vote score seems normally distributed (mean = 6.2, SD = 0.8).Q1: Did high budget movies achieve high ROI?I plotted a scatter plot (top), ROI distribution by budget level (bottom-left), and median value by budget level (bottom right).It is hard to find a relationship in the scatter plot. But, the boxplot by the budget level shows that ROI values of low budget movies span a wide range, up to 500, the largest ROI value in the dataset. The larger budget groups have the narrower spread of ROIs. As well, the comparison of median values shows that the low and high budget levels have a little higher median values than the moderate and little high budget levels.Meanwhile, I created a contigency table and plotted it as below.It is found that low budget movies achieve the low and high level ROIs more frequently than the moderate and little high level ROIs. As opposed to it, high budget movies obtain the moderate and little high level ROIs more frequenlty.Q2: Did specific genres lead to high ROI?To compare ROIs between the genres, I calculated two types of means:  Simple mean: Simply add up all ROIs and divide it with the total number of movies involving that genre.  Weighted mean: Assign weights based on the number genre types involved in a movie. A ROI is multiplied by 1/number_of_genres and the sum is divided by the sum of 1/number_of_genres values.For example, let’s assume we have a long format dataframe of three movies as below:            id      genre      ROI      number of genres                  1      drama      3      2              1      comedy      3      2              2      comedy      5      1              3      drama      2      1      Simple means are:  drama, (3+2) / 2 = 2.5  comedy, (3+5) / 2 = 4Weighted means are:  drama, (3x1/2+2x1/1) / (1/2+1) = (1.5+2) / 1.5 = 2.33  comedy, (3x1/2+5x1/1) / (1/2+1) = (1.5+5) / 1.5 = 4.33The below shows results of the two methods. The results are not exactly the same (especailly, ‘Horror’ has a larger difference between the two results than other genrens), but  order is the same.The results hints that ‘Horror’ tends to bring quite larger ROIs compared to the others. The next profitable genre is ‘Music’ by round 4 or 5 smaller ROI than Horror. ROI values gradually decrease after ‘Music, and thus the difference between ‘Music’ and ‘History’ is only about 3.5.Although I believe that I obtained an acceptable finding with my approach, it is a shame that effect of combination of genres was not studied. This issue will be discussed a bit more in ‘Limitations’ section at the end.Q3: Did high score movies achieve high ROI?The chart shows that the high ROI level tends to bring a little higher median of vote scores (0.4 higher than the little high level, and 0.7 than the low level).ConclusionsSummaryWith the exploration, I found association between 1) budget size and ROI, 2) genres and ROI, and 3) vote average score and ROI. To sum up:  I found contrary behavior between the low and high budget groups, in terms of their ROIs. Low budget movies are more likely to be big fail or big success, and high budget movies tend to end up in the middle. This behavior seems a consequence. Considering different capabilities in reaching the audience and the definition of ROI, this behavior seems natural.  The genre type that resulted in the highest ROI on average is ‘Horror’, and the smallest ROI, ‘History’,  among 17 genre types (excluding ‘Western’, ‘Documentary’, ‘Foreign’, and ‘TV Movie’).  It was found that the higher ROI level group tend to associate with higher mean of the vote average.Based on these findings, it is considered that production of low budget, horror movies that receive high score from the audience is likely to lead to high ROI.LimitationsThe exploration has several limitations as follows:  Only about a third of the original datapoints were used in the analysis, since the rest had missing or outlying values. As well, there was no further work to check if any pattern exists in missing or outlying values.  In finding association between genres and ROIs, potential effect of genre combination was not explored, and it makes the result somewhat insecure. For instance, let’s assume the following dataset of four movies:            id      genres      ROI                  1      Drama      2              2      Drama, Family      8              3      Drama, Horror      8              4      Family      2      Calculation of weighted means (Drama, 5; Family, 4; Horor, 8) indicates that the highest ROI genre is Horror. However, it is not sure if this is the power of the Horror genre or result of pairing with Drama, since it does not have movies that involve only Horror.",
        "url": "/blog-site//2022/06/01/what-are-associated-with-profitable-movies.html"
      }
      
    
  };
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/2.3.6/lunr.min.js"></script>
<script src="/blog-site/assets/js/search.js"></script></section>
</article>

    </div>
    


<footer class="site-footer">
	<p class="text">© 2022 ・ Jisoo Lee
</p>
</footer>


  </body>
</html>
