<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Implementing Hierarchical and KMeans Clustering on Principal Components | Jisoo Lee</title>
	<meta name="description"
		content="Let’s say, you want to identify groups of customers to treat each group with different strategies. If you have a dataset containing features describing custo...">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<!-- CSS -->
	<link rel="stylesheet" href="/blog-site/assets/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/blog-site/assets/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="/blog-site/2022/09/18/implementing-hierarchical-and-kmeans-clustering-on-principal-components.html">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Jisoo Lee"
		href="/blog-site/feed.xml" />

	<!-- Font Awesome -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css"
		integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet"
		type="text/css">
	

	<!-- KaTeX -->
	
	<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css"
		integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">

	<script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js"
		integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ"
		crossorigin="anonymous"></script> -->

	 <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"> -->
	 
	 <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
	 <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
	 <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
	 <script>
		 document.addEventListener("DOMContentLoaded", function() {
			 renderMathInElement(document.body, {
				 // ...options...
			 });
		 });
	 </script>
	

	<!-- Google Analytics -->
	

<!-- Begin section for dataframe table formatting -->
<style type="text/css">
    table.dataframe {
        width: 100%;
        height: 240px;
        display: block;
		overflow: auto;
        font-family: Arial, sans-serif;
        font-size: 0.7em;
        line-height: 1.2;
        text-align: center;
    }
    table.dataframe th {
      font-weight: bold;
      padding: 0.3em;
    }
    table.dataframe td {
      padding: 0.3em;
    }
    table.dataframe tr:hover {
      background: #b8d1f3; 
    }


	table.dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    table.dataframe tbody tr th {
        vertical-align: top;
    }

    table.dataframe thead th {
        text-align: left;
    }

	.notebook_output pre {
		font-size: 0.8em;
		border: 1;
		line-height: 1.2;
	}

	.highlighter-rouge pre{
		margin-bottom: 1.5em;
		padding: 1.2em;
	}

</style>
<!-- End section for dataframe table formatting -->

</head>
  <body>
    <header class="site-header">
	<div class="branding">
		
		<h1 class="site-title">
			<a href="/blog-site/">Jisoo Lee</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
			
			
			
			
			
			<li>
				<a class="page-link" href="/blog-site/about/">
					About
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled  -->
			














<li>
	<a href="https://github.com/jisooleeworks" title="Follow on GitHub">
		<i class="fab fa-fw fa-github"></i>
	</a>
</li>































            <!-- Search bar -->
            
		</ul>
	</nav>

</header>

    <div class="content">
      <article >
  <header style="background-image: url('/blog-site/')">
    <h1 class="title">Implementing Hierarchical and KMeans Clustering on Principal Components</h1>
    
    <p class="meta">
      September 18, 2022
      
    </p>
  </header>
  <section class="post-content"><p>Let’s say, you want to identify groups of customers to treat each group with different strategies. If you have a dataset containing features describing customers’ various aspects such as age, occupation, living area, spending behavior and so on, clustering methods can help you figure it out. Hierarchical clustering and KMeans clustering combined with PCA (Principal Component Analysis) are often used in various clustering methods. In this post, I will present how to implement hierarchical clustering, KMeans clustering, and both on PCA features, using SciPy and Scikit-learn libraries in Python.</p>

<p>This description is guided by Asish Biswas’ worked example [1, 2, 3], which aims to show you how to apply the STP (Segmentation, Targeting, Positioning) marketing framework. The dataset used contains demographic information of 2000 customers, as follows:</p>
<ul>
  <li>Sex – categorical, 0: male, 1: female</li>
  <li>Marital status – categorical, 0: single, 1: non-single</li>
  <li>Age – numerical</li>
  <li>Education – categorical, 0: unknown, 1: high school, 2: university, 3: graduate</li>
  <li>Income – numerical</li>
  <li>Occupation – categorical, 0: unemployed, 1: official, 2: management</li>
  <li>Settlement size – categorical, 0: small city, 1: mid city, 2: big city</li>
</ul>

<p>I will first talk about initial data exploration and do preprocessing. Then, I will show implementation of the clusterings one by one and then compare results of them. You can find full implementation and code in <a href="https://github.com/jisooleeworks/blog-code/blob/main/implement_clustering.ipynb">my github repository</a>.</p>

<h1 id="data-exploration-and-preprocessing">Data Exploration and Preprocessing</h1>
<p>Let’s first look at several rows of the dataset to get familar with it.</p>

<table style="border: 1; height: 180px;" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sex</th>
      <th>Marital status</th>
      <th>Age</th>
      <th>Education</th>
      <th>Income</th>
      <th>Occupation</th>
      <th>Settlement size</th>
    </tr>
    <tr>
      <th>ID</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>100000001</th>
      <td>0</td>
      <td>0</td>
      <td>67</td>
      <td>2</td>
      <td>124670</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>100000002</th>
      <td>1</td>
      <td>1</td>
      <td>22</td>
      <td>1</td>
      <td>150773</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>100000003</th>
      <td>0</td>
      <td>0</td>
      <td>49</td>
      <td>1</td>
      <td>89210</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>100000004</th>
      <td>0</td>
      <td>0</td>
      <td>45</td>
      <td>1</td>
      <td>171565</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>100000005</th>
      <td>0</td>
      <td>0</td>
      <td>53</td>
      <td>1</td>
      <td>149031</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>I’ve drawn plots to check distribution of each variable and relationship between pairs of the variables.</p>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/org_pairplot.png" alt="Individual Distribution and Pairwise Relationship" /></p>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/org_correlation_heatmap.png" alt="Correlation Matrix" /></p>

<p>The plots reveal existence of several correlations including relationship between the age and the education and between the occupation and the income.</p>

<p>Since distance-based clustering methods are affected by difference of variable scales, I applied  standardization to the dataset. I picked the standardization as I will do PCA.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">.</span><span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">data_sd</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="hierarchical-clustering">Hierarchical Clustering</h1>
<p>I apply euclidean distance for a distance measure and Ward´s linkage for a linkage function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.cluster</span> <span class="kn">import</span> <span class="n">hierarchy</span>
<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">distance</span>

<span class="n">linkages_sd</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="p">.</span><span class="n">linkage</span><span class="p">(</span><span class="n">data_sd</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'ward'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">hierarchy</span><span class="p">.</span><span class="n">dendrogram</span><span class="p">(</span><span class="n">linkages_sd</span><span class="p">,</span>
                     <span class="n">show_leaf_counts</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                     <span class="n">no_labels</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Hierarchical Clustering Dendrogram"</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/hc_dendrogram.png" alt="Hierarchical Clustering Dendrogram" /></p>

<p>The  dendrogram hints that making four groups may be reasonable.</p>

<h1 id="kmeans-clustering">KMeans Clustering</h1>
<p>To determine the optimal number of clusters, I draw an elbow plot that tells the degree of variance explained according to the different number of clusters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cluster</span>

<span class="n">distortions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">num_clusters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">num_clusters</span><span class="p">:</span>
  <span class="n">centroids</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">inertia</span> <span class="o">=</span> <span class="n">cluster</span><span class="p">.</span><span class="n">k_means</span><span class="p">(</span><span class="n">data_sd</span><span class="p">,</span> <span class="n">n_clusters</span> <span class="o">=</span> <span class="n">i</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
  <span class="n">distortions</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">inertia</span><span class="p">)</span>
  <span class="n">labels_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

  <span class="n">elbow_plot</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'num_clusters'</span><span class="p">:</span> <span class="n">num_clusters</span><span class="p">,</span> 
                            <span class="s">'distortions'</span><span class="p">:</span> <span class="n">distortions</span><span class="p">})</span>
  <span class="n">sns</span><span class="p">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'Number of clusters'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'Sum of squared distances'</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elbow_plot</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">num_clusters</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"KMeans Clustering Elbow Graph"</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/km_elbow.png" alt="Elbow Graph of KMeans Clustering" /></p>

<p>If comparing the magnitude of the slope between two numbers, it tends to remain the same from four. So, it is considered best to make four groups.</p>

<h1 id="pca">PCA</h1>
<p>In identifying principal components, let’s first import the sklearn library and create a PCA object with the scaled dataset.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span> 

<span class="n">pca</span> <span class="o">=</span> <span class="n">decomposition</span><span class="p">.</span><span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_sd</span><span class="p">)</span>
</code></pre></div></div>

<p>We draw a plot that shows the degree of the explained variance accumulated with additional component.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(),</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of components'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Cumulative explained aariance'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Cumulative Explained Variance by Number of Principal Components"</span><span class="p">);</span>
</code></pre></div></div>
<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/pca_explanation.png" alt="Correlation Matrix for Principal Components and Original Features" /></p>

<p>In the above plot, we can see that the first three components explain 80% of the variance, which is considered acceptable [4]. So, I make an instance of the model with three for a number of components and fit it on the dataset. Then, I make a dataframe and heatmap with it to see a loading score of each variable for each component.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">decomposition</span><span class="p">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_sd</span><span class="p">)</span>

<span class="n">df_pca_components</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">values</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s">'component 1'</span><span class="p">,</span> <span class="s">'component 2'</span><span class="p">,</span> <span class="s">'component 3'</span><span class="p">])</span>

<span class="n">s</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">heatmap</span><span class="p">(</span>
    <span class="n">df_pca_components</span><span class="p">,</span>
    <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s">'RdBu_r'</span><span class="p">,</span>
    <span class="n">annot</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Correlation Matrix for Principal Components and Original Features'</span><span class="p">);</span>
</code></pre></div></div>
<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/pca_correlation_heatmap.png" alt="Correlation Matrix for Principal Components and Original Features" /></p>

<p>With the above heatmap, we can see that:</p>
<ul>
  <li>Component 1 has a positive association with Income, Occupation, and Settlement Size</li>
  <li>Component 2 has a positive association with Sex, Martital Status, and Education</li>
  <li>Component 3 has a positive association with Age and a negative association with Martital Status and Occupation.</li>
</ul>

<h1 id="hierarchical-clustering-and-kmeans-clustering-on-pca-features">Hierarchical Clustering and KMeans Clustering on PCA Features</h1>
<p>Let’t run a hierarchical clustering model and KMeans on the three principal components.</p>

<p>I first obtain a new dataset by transforming the original scaled dataset on the three principal components and then run a model on it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca_scores</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data_sd</span><span class="p">)</span>
<span class="n">linkages_pca</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="p">.</span><span class="n">linkage</span><span class="p">(</span><span class="n">pca_scores</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'ward'</span><span class="p">)</span>
</code></pre></div></div>
<p>I do not bring a dendrogram of the result here; you can find it in <a href="https://github.com/jisooleeworks/blog-code/blob/main/implement_clustering.ipynb">my github repository</a>. If you see it, you will find that making four groups is a reasonable decision.</p>

<p>I plot the segments with respect to the first two components as below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">labels_hc_pca</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="p">.</span><span class="n">fcluster</span><span class="p">(</span><span class="n">linkages_pca</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s">'maxclust'</span><span class="p">)</span>

<span class="n">df_hc_pca</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca_scores</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_hc_pca</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span> <span class="o">=</span> <span class="p">[</span><span class="s">'component 1'</span><span class="p">,</span> <span class="s">'component 2'</span><span class="p">,</span> <span class="s">'component 3'</span><span class="p">]</span>
<span class="n">df_hc_pca</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels_hc_pca</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">df_hc_pca</span><span class="p">[</span><span class="s">'component 1'</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">df_hc_pca</span><span class="p">[</span><span class="s">'component 2'</span><span class="p">],</span>
    <span class="n">hue</span><span class="o">=</span><span class="n">df_hc_pca</span><span class="p">[</span><span class="s">'label'</span><span class="p">],</span>
    <span class="n">palette</span><span class="o">=</span><span class="p">[</span><span class="s">'b'</span><span class="p">,</span><span class="s">'m'</span><span class="p">,</span><span class="s">'g'</span><span class="p">,</span><span class="s">'r'</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Hierarchical Clustering on Principle Components'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/hc_pca_scatter.png" alt="Hierarchical Clustering on Principle Components" /></p>

<p>I think the segments are well seperated in overall, although the label 4 overlaps the label 2 and 3 a bit.</p>

<h1 id="kmeans-clustering-on-pca-features">KMeans Clustering on PCA Features</h1>
<p>Finally, we run a KMeans clustering model on the three principal components. Based on the elbow graph that you can find in the github repository, I pick four as a number of clusters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">centroids</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">inertia</span> <span class="o">=</span> <span class="n">cluster</span><span class="p">.</span><span class="n">k_means</span><span class="p">(</span><span class="n">pca_scores</span><span class="p">,</span> <span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/km_pca_scatter.png" alt="Hierarchical Clustering on Principle Components" /></p>

<h1 id="comparison">Comparison</h1>
<p>Now, we have four different versions of segmentation on the dataset. Below is a summary of each version (mean value of each variable for each cluster), in order of hierarchical clustering, KMeans clusteirng, hierarchical clustering with PCA, and KMeans clustering with PCA.</p>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/four_results.png" alt="Correlation Matrix for Principal Components and Original Features" width="60%" height="60%" /></p>

<p>Before getting the above, I’ve printed out a raw result of each version, done some eyeballing, and re-ordered it based on the age and income variable so that:</p>

<ul>
  <li>Label 1, the youngest and the third level of income</li>
  <li>Label 2, in the middle level in age and the lowest level of income</li>
  <li>Label 3, in the mdidle level in the age and the second level of income</li>
  <li>Label 4, the eldest and the the top level of income</li>
</ul>

<p>This segmentatation accord with all the four versions; e.g., the youngest group’s income rank is third in all the versions. The mean age values are simiar each other. But the mean income values are fairly much different. As well, we can see discordance between the versions in the other variables.</p>

<p>I am going to do further analysis on the segmentation done with the Kmeans clustering on principal components.</p>

<h1 id="exploration-of-segments">Exploration of Segments</h1>
<p>In this section, I will look into the segments created with the Kmeans clustering on principal components to identify characteristics of each of them.</p>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/seg_age_income.png" alt="Age and Income Distribution by Segment" /></p>

<p>If we compare the median values of the Age, Segment 4 is distinctively high, Segment 2 and 3 are similar, and Segment 1 is the lowest. However, we can see their overlapping with the long whiskers and outliers. The Income plot confirms that Segment 4 is higher than the others, especially Segment 3. However, we can see the wide range of Segment 4 with some extreme outliers on the top and the minimum value lower than Segment 3’s.</p>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/seg_uni_count.png" alt="Count by Segment" /></p>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/seg_bi_count1.png" alt="Count by Two Variables and Segment 1" /></p>

<p><img src="/blog-site/assets/img/2022-09-18-implementing-hierarchical-and-kmeans-clustering-on-principal-components/seg_bi_count2.png" alt="Count by Two Variables and Segment 2" /></p>

<p>The most distinctive aspect of Segment 1 is that they are mostly high school graduated non-single women in their late 20s. Two-thirds of them live in the small city, half of which have no job. A third live in the mid-sized city with ‘official’ jobs.</p>

<p>Most people of Segment 2 are single living in the small city in the mid 30s. Two-thirds are male and a third, female. Two-thirds are unemployed.</p>

<p>People of Segment 3 are male living the mid or big-sized city with job. The majority is single.</p>

<p>All people of Segment 4 were graduated from university or the higher. Two-thirds are non-single, and a third, single. The sex ratio is one to one. They distributed to all the sizes of the city although more people live in the mid or big-sized city.</p>

<h1 id="wrapping-up">Wrapping Up</h1>
<p>I have showed how to implement the main clustering methods, hierarchical and KMean clustering combined with PCA. When use the hierarchical clustering, you find out the appropriate number of clusters by looking at a dendrogram. If use the KMean clustering, the elbow graph can help you pick the number of clusters. We have found that the obtained segments are distintive each other, and it suggests usefulness of the clustering methods in segmenting customers.</p>

<h1 id="references">References</h1>
<ol>
  <li>Asish Biswas. https://medium.com/towards-data-science/customer-segmentation-with-python-implementing-stp-framework-part-1-5c2d93066f82</li>
  <li>Asish Biswas. https://medium.com/towards-data-science/customer-segmentation-with-python-implementing-stp-framework-part-2-689b81a7e86d</li>
  <li>Asish Biswas. https://medium.com/towards-data-science/customer-segmentation-with-python-implementing-stp-framework-part-3-e81a79181d07</li>
  <li>Elitsa Kaloyanova. What Is Principal Components Analysis? https://365datascience.com/tutorials/python-tutorials/principal-components-analysis/</li>
</ol>

</section>
  

</article>

<!-- Disqus -->


<!-- Post navigation -->


    </div>
    
<script src="/blog-site/assets/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">© 2022 ・ Jisoo Lee
</p>
</footer>


  </body>
</html>
